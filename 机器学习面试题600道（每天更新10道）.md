# 两个月刷完机器学习（包含深度学习）600题。（题目来源于BAT等厂面试题）  每天10道。  
## 第一天    
1、以下哪种方法属于判别式模型(discriminative model)（ ）    
A 隐马模型(HMM)    
B 朴素贝叶斯    
C LDA    
D 支持向量机    
正确答案是：D    
 已知输入变量x，判别模型(discriminative model)通过求解条件概率分布P(y|x)或者直接计算y的值来预测y。生成模型（generative model）通过对观测值和标注数据计算联合概率分布P(x,y)来达到判定估算y的目的。
   常见的判别模型有线性回归（Linear Regression）,逻辑回归（Logistic Regression）,支持向量机（SVM）, 传统神经网络（Traditional Neural Networks）,线性判别分析（Linear Discriminative Analysis），条件随机场（Conditional Random Field）；常见的生成模型有朴素贝叶斯（Naive Bayes）, 隐马尔科夫模型（HMM）,贝叶斯网络（Bayesian Networks）和隐含狄利克雷分布（Latent Dirichlet Allocation）。
   A选项的隐马尔科夫模型和 B选项的朴素贝叶斯属于生成模型。        C选项的LDA，如果是指Linear Discriminative   Analysis，那么属于判别模型，如果是指 Latent Dirichlet Allocation，那么属于生成模型。    D选项的支持向量机属于判别模型。    
2、以P(w)表示词条w的概率，假设已知P（南京）=0.8，P（市长）=0.6，P（江大桥）=0.4：P（南京市）=0.3，P（长江大桥）=0.5：如果假设前后两个词的出现是独立的，那么分词结果就是（ ）       
A 南京市*长江*大桥    
B 南京*市长*江大桥    
C 南京市长*江大桥    
D 南京市*长江大桥   
正确答案是： B    
解析：
    该题考察的是最大概率分词，其基本思想是：一个待切分的汉字串可能包含多种分词结果，将其中概率最大的作为该字串的分词结果。若某候选词在训练语料中未出现，其概率为0。    
    A分词结果的概率为P(A)=P(南京市)*P(长江)*P(大桥)，由于“长江”未在语料中出现，所以P(长江)=0，从而P(A)=0;    
    同理可以算出B, C, D分词结果的概率分别是：    
    P(B)=P(南京)*P(市长)*P(江大桥)=0.8*0.6*0.4=0.192；    
    P(C)=P(南京市长)*P(江大桥)=0*0.4=0；    
    P(D)=P(南京市)*P(长江大桥)=0.3*0.5=0.15。    
    因为P(B)最大，所以为正确的分词结果。    
3、基于统计的分词方法为（ ）    
A
正向量最大匹配法    
B
逆向量最大匹配法    
C
最少切分    
D
条件随机场    
正确答案：D    
中文分词的基本方法可以分为基于语法规则的方法、基于词典的方法和基于统计的方法。
    基于语法规则的分词法基本思想是在分词的同时进行句法、语义分析, 利用句法信息和语义信息来进行词性标注, 以解决分词歧义现象。因为现有的语法知识、句法规则十分笼统、复杂, 基于语法和规则的分词法所能达到的精确度远远还不能令人满意, 目前这种分词系统应用较少。    
    在基于词典的方法中，可以进一步分为最大匹配法，最大概率法，最短路径法等。最大匹配法指的是按照一定顺序选取字符串中的若干个字当做一个词，去词典中查找。根据扫描方式可细分为：正向最大匹配，反向最大匹配，双向最大匹配，最小切分。最大概率法指的是一个待切分的汉字串可能包含多种分词结果，将其中概率最大的那个作为该字串的分词结果。最短路径法指的是在词图上选择一条词数最少的路径。    
    基于统计的分词法的基本原理是根据字符串在语料库中出现的统计频率来决定其是否构成词。词是字的组合，相邻的字同时出现的次数越多, 就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映它们成为词的可信度。常用的方法有HMM（隐马尔科夫模型），MAXENT（最大熵模型），MEMM（最大熵隐马尔科夫模型），CRF（条件随机场）。    
    本题中，基于统计的方法为条件随机场。ABC三个选项为基于词典的方法。

4、下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）    
A
特征灵活    
B
速度快     
C
可容纳较多上下文信息    
D
全局最优   
正确答案是： B    
HMM模型是对转移概率和表现概率直接建模，统计共现概率。而MEMM模型是对转移概率和表现概率建立联合概率，统计时统计的是条件概率。CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。MEMM容易陷入局部最优，是因为MEMM只在局部做归一化。CRF模型中，统计了全局概率，在做归一化时，考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。
    CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，特征设计灵活。CRF需要训练的参数更多，与MEMM和HMM相比，它存在训练代价大、复杂度高的缺点。    
5、隐马尔可夫模型（HMM），设其观察值    
空间为![](https://ws1.sinaimg.cn/large/006tNbRwly1fvfqmcxaayj304q00kwea.jpg)
状态空间为![](https://ws3.sinaimg.cn/large/006tNbRwly1fvfqmriotlj304n00kq2q.jpg)
如果用维特比算法(Viterbi algorithm)进行解码，时间复杂度为（ ）    
A
O(NK)    
B
O(NK^2)      
C
O(N^2K)        
D
以上都不是      
正确答案是：D     
![](https://ws2.sinaimg.cn/large/006tNbRwly1fvfrnahl9rj30pm08radd.jpg)    



6、在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（ ）（假设precision=TP/(TP+FP),recall=TP/(TP+FN)。）    
A
Accuracy:(TP+TN)/all    
B
F-value:2*recall*precision/(recall+precision)    
C
G-mean:sqrt(precision*recall)    
D
AUC:ROC曲线下面积    
正确答案是：A     
对于分类器，主要的评价指标有precision，recall，F-score，以及ROC曲线等。
    在二分类问题中，我们主要关注的是测试集的正样本能否正确分类。当样本不均衡时，比如样本中负样本数量远远多于正样本，此时如果负样本能够全部正确分类，而正样本只能部分正确分类，那么(TP+TN)可以得到很高的值，也就是Accuracy是个较大的值，但是正样本并没有取得良好的分类效果。因此A选项是不合理的。在样本不均衡时，可以采用BCD选项方法来评价。    
7、下面关于ID3算法中说法错误的是（ ）    
A
ID3算法要求特征必须离散化    
B
信息增益可以用熵，而不是GINI系数来计算    
C
选取信息增益最大的特征，作为树的根节点    
D
ID3算法是一个二叉树模型    
正确答案是：D    
D3算法（IterativeDichotomiser3迭代二叉树3代）是一个由RossQuinlan发明的用于决策树的算法。可以归纳为以下几点：
使用所有没有使用的属性并计算与之相关的样本熵值
选取其中熵值最小的属性
生成包含该属性的节点

D3算法对数据的要求：
1)所有属性必须为离散量；
2)所有的训练例的所有属性必须有一个明确的值；
3)相同的因素必须得到相同的结论且训练例必须唯一。    
8、如下表是用户是否使用某产品的调查结果（ ）   
请计算年龄、地区、学历、收入中对用户是否使用调查产品信息增益最大的属性。
（）  ![](https://ws3.sinaimg.cn/large/006tNbRwly1fvfqnjvjwxj302d00n0si.jpg)      

![](https://ws4.sinaimg.cn/large/006tNbRwly1fvfqnsrc7zj30g104amxl.jpg)    
A 年龄     
B 地区     
C 学历     
D 收入     
正确答案是：C    
![](https://ws2.sinaimg.cn/large/006tNbRwly1fvfrsgxzvwj30pb04rdhk.jpg)    

9、在其它条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题（ ）    
A
增加训练集数量    
B
减少神经网络隐藏层节点数    
C
删除稀疏的特征    
D
SVM算法中使用高斯核/RBF核代替    
正确答案是：D    
机器学习中发生过拟合的主要原因有：    
（1）使用过于复杂的模型；
（2）数据噪声较大；
（3）训练数据少。    
    由此对应的降低过拟合的方法有：    
（1）简化模型假设，或者使用惩罚项限制模型复杂度；
（2）进行数据清洗，减少噪声；
（3）收集更多训练数据。    

本题中，A对应于增加训练数据，B为简化模型假设，C为数据清洗。D选项中，高斯核的使用增加了模型复杂度，容易引起过拟合。选择合适的核函数以及软边缘参数C就是训练SVM的重要因素。一般来讲，核函数越复杂，模型越偏向于过拟合；C越大模型越偏向于过拟合，反之则拟合不足。    
10、如果线性回归模型中的随机误差存在异方差性，那么参数的OLS估计量是（ ）    
A
无偏的，有效的    
B
无偏的，非有效的    
C
有偏的，有效的    
D 
有偏的，非有效的    
正确答案是： B    
   OLS即普通最小二乘法。由高斯—马尔可夫定理，在给定经典线性回归的假定下，最小二乘估计量是具有最小方差的线性无偏估计量。根据证明过程可知，随机误差中存在异方差性不会影响其无偏性，而有效性证明中涉及同方差性，即异方差会影响参数OLS估计量的有效性。    

## 第二天
1、一个二进制源X发出符号集为{-1,1}，经过离散无记忆信道传输，由于信道中噪音的存在，接收端Y收到符号集为{-1,1,0}。已知P(x=-1)=1/4，P(x=1)=3/4，P(y=-1|x=-1)=4/5，P(y=0|x=-1)=1/5，P(y=1|x=1)=3/4，P(y=0|x=1)=1/4，求条件熵H(Y|X)（ ）    
A
0.2375    
B
0.3275    
C
0.5273    
D
0.5372   
正确答案是：A    
由H(Y|X)= -∑P(X,Y)logP(Y|X)= -∑P(Y|X)P(X)logP(Y|X)，将(y=-1,x=-1), (y=0,x=-1), (y=1,x=1), (y=0,x=1)四种情况带入公式求和，得到H(Y|X)≈-(-0.01938-0.03495-0.07028-0.11289)=0.2375。    

2、Fisher线性判别函数的求解过程是将M维特征矢量投影在（ ）中进行求解。    
A
M-1维空间    
B
一维空间    
C
三维空间    
D
二维空间
正确答案是： B    
Fisher线性判别函数是将多维空间中的特征矢量投影到一条直线上，也就是把维数压缩到一维。寻找这条最优直线的准则是Fisher准则：两类样本在一维空间的投影满足类内尽可能密集，类间尽可能分开，也就是投影后两类样本均值之差尽可能大，类内部方差尽可能小。一般而言，对于数据分布近似高斯分布的情况，Fisher线性判别准则能够得到很好的分类效果。    
3、类域界面方程法中，不能求线性不可分情况下分类问题近似或精确解的方法是（ ）    
A
势函数法    
B
基于二次准则的H-K算法    
C
伪逆法    
D
感知器算法
正确答案是：D    
线性分类器的设计就是利用训练样本集建立线性判别函数式，也就是寻找最优的权向量的过程。求解权重的过程就是训练过程，训练方法的共同点是，先给出准则函数，再寻找是准则函数趋于极值的优化方法。ABC方法都可以得到线性不可分情况下分类问题近似解。感知器可以解决线性可分的问题，但当样本线性不可分时，感知器算法不会收敛。    
4、下列哪个不属于CRF模型对于HMM和MEMM模型的优势    
A
特征灵活    
B
速度快     
C
可容纳较多上下文信息    
D
全局最优   
正确答案是： B    
  HMM模型是对转移概率和表现概率直接建模，统计共现概率。而MEMM模型是对转移概率和表现概率建立联合概率，统计时统计的是条件概率。CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。MEMM容易陷入局部最优，是因为MEMM只在局部做归一化。CRF模型中，统计了全局概率，在做归一化时，考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。    
    CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，特征设计灵活。CRF需要训练的参数更多，与MEMM和HMM相比，它存在训练代价大、复杂度高的缺点。    
5、Nave Bayes是一种特殊的Bayes分类器,特征变量是X,类别标签是C,它的一个假定是（）    
A
各类别的先验概率P(C)是相等的    
B
以0为均值，sqr(2)/2为标准差的正态分布    
C
特征变量X的各个维度是类别条件独立随机变量    
D
P(X|C)是高斯分布    
正确答案：C    
朴素贝叶斯的基本假设就是每个变量相互独立。    
6、在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计（）    
A
EM算法    
B
维特比算法    
C
前向后向算法    
D
极大似然估计
正确答案是：D    
EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法
维特比算法： 用动态规划解决HMM的预测问题，不是参数估计
前向后向算法：用来算概率
极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数
注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。    
7、假定某同学使用Naive Bayesian（NB）分类模型时，不小心将训练数据的两个维度搞重复了，那么关于NB的说法中不正确的是？    
A
模型效果相比无重复特征的情况下精确度会降低    
B
如果所有特征都被重复一遍，得到的模型预测结果相对于不重复的情况下的模型预测结果一样    
C
当两列特征高度相关时，无法用两列特征相同时所得到的结论来分析问题   
正确答案是： B    
朴素贝叶斯的条件就是每个变量相互独立。在贝叶斯理论系统中，都有一个重要的条件独立性假设：假设所有特征之间相互独立，这样才能将联合概率拆分。    

此外，若高度相关的特征在模型中引入两次, 这样增加了这一特征的重要性, 则它的性能因数据包含高度相关的特征而下降。正确做法是评估特征的相关矩阵，并移除那些高度相关的特征。    
8、以下哪些方法不可以直接来对文本分类？    
A
Kmeans    
B
决策树     
C
支持向量机     
D
KNN  
正确答案：A
Kmeans是聚类方法，典型的无监督学习方法。分类是监督学习方法，BCD都是常见的分类方法。    
9、已知一组数据的协方差矩阵P,下面关于主分量说法错误的是（）    
A
主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小    
B
在经主分量分解后,协方差矩阵成为对角矩阵     
C
主分量分析就是K-L变换    
D
主分量是通过求协方差矩阵的特征值得到    
正确答案是：C    
K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。    
10、关于logit 回归和SVM 不正确的是（ ）    
A
Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。    
B
Logit回归的输出就是样本属于正类别的几率，可以计算出概率。    
C
SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。    
D
SVM可以通过正则化系数控制模型的复杂度，避免过拟合。    
正确答案是：A    
Logit回归目标函数是最小化后验概率，Logit回归可以用于预测事件发生概率的大小，SVM目标是结构风险最小化，SVM可以有效避免模型过拟合。    



## 第三天
1、以下不属于影响聚类算法结果的主要因素有（）    
A
已知类别的样本质量    
B
分类准则    
C
特征选取    
D
模式相似性测度   
正确答案是：A    
都已知了，就不必再进行聚类了。    
2、模式识别中，不属于马式距离较之于欧式距离的优点的是（ ）    
A
平移不变性    
B
尺度不变性    
C
考虑了模式的分布  
正确答案是：A    
欧氏距离定义：欧氏距离（ Euclidean distance）是一个通常采用的距离定义，它是在m维空间中两个点之间的真实距离。
在二维和三维空间中的欧式距离的就是两点之间的距离，二维的公式是
d = sqrt((x1-x2)^+(y1-y2)^)
三维的公式是
d=sqrt(x1-x2)^+(y1-y2)^+(z1-z2)^)
推广到n维空间，欧式距离的公式是
d=sqrt( ∑(xi1-xi2)^ ) 这里i=1,2..n
xi1表示第一个点的第i维坐标,xi2表示第二个点的第i维坐标
n维欧氏空间是一个点集,它的每个点可以表示为(x(1),x(2),...x(n)),其中x(i)(i=1,2...n)是实数,称为x的第i个坐标,两个点x和y=(y(1),y(2)...y(n))之间的距离d(x,y)定义为上面的公式.
欧氏距离看作信号的相似程度。距离越近就越相似，就越容易相互干扰，误码率就越高。        
马氏距离是由印度统计学家马哈拉诺比斯(P. C. Mahalanobis)提出的，表示数据的协方差距离。它是一种有效的计算两个未知样本集的相似度的方法。与欧式距离不同的是它考虑到各种特性之间的联系（例如：一条关于身高的信息会带来一条关于体重的信息，因为两者是有关联的），并且是尺度无关的(scale-invariant)，即独立于测量尺度。    
3、影响基本K-均值算法的主要因素有（）    
A
样本输入顺序    
B
模式相似性测度    
C
聚类准则    
正确答案：B    
有三个因素，样本输入顺序和初试类中心选取，对模型在误差下降图中造成不同的最优。以及模型相似性测度，是下降到最优的途径。    
4、在统计模式分类问题中，当先验概率未知时，可以使用（）    
A
最小损失准则    
B
最小最大损失准则    
C
最小误判概率准则  
正确答案：B 
在统计模式分类问题中，当先验概率未知时，可以使用最小最大损失准则、N-P判决。    
5、如果以特征向量的相关系数作为模式相似性测度，则影响聚类算法结果的主要因素有（ ）     
A
已知类别样本质量    
B
分类准则    
C
量纲    
正确答案是： B    
如果以特征向量的相关系数作为模式相似性测度，则影响聚类算法结果的主要因素有：分类准则、特征选取。

6、以下属于欧式距离特性的有（）    
A
旋转不变性    
B
尺度缩放不变性    
C
不受量纲影响的特性
正确答案是：A    
欧式距离特性有：平移不变性、旋转不变性。    
马式距离特性有：平移不变性、旋转不变性、尺度缩放不变性、不受量纲影响的特性、考虑了模式的分布。    
7、以下( )不属于线性分类器最佳准则？    
A
感知准则函数    
B
贝叶斯分类    
C
支持向量机    
D
Fisher准则    
正确答案是： B    
线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。
感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。
支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题）
Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。
根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵 Sw 和类间离散矩阵 Sb 实现。    
8、一监狱人脸识别准入系统用来识别待进入人员的身份，此系统一共包括识别4种不同的人员：狱警，小偷，送餐员，其他。下面哪种学习方法最适合此种应用需求：    
A
二分类问题    
B
多分类问题    
C
层次聚类问题    
D
k-中心点聚类问题    
E
回归问题    
F
结构分析问题   
正确答案是： B    
识别4种，那么就是多分类。
9、关于 logit 回归和 SVM 不正确的是（）    
A
Logit回归目标函数是最小化后验概率    
B
Logit回归可以用于预测事件发生概率的大小    
C
SVM目标是结构风险最小化    
D
SVM可以有效避免模型过拟合    
正确答案是：A    
A. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。而最小化后验概率是朴素贝叶斯算法要做的。A错误    
B. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确    
C. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。    
D. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。    
10、有两个样本点，第一个点为正样本,它的特征向量是(0,-1);第二个点为负样本,它的特征向量是(2,3),从这两个样本点组成的训练集构建一个线性SVM分类器的分类面方程是( )    
A
2x+y=4    
B
x+2y=5    
C
x+2y=3    
D
2x-y=0    
正确答案是：C    
这道题简化了，对于两个点来说，最大间隔就是垂直平分线，因此求出垂直平分线即可。斜率是两点连线的斜率的负倒数-1/((-1-3)/(0-2)) = -1/2, 可得y=-(1/2)x + c, 过中点((0+2)/2, (-1+3)/2) = (1, 1), 可得c=3/2, 故选C.




## 第四天:    
1、下列时间序列模型中,哪一个模型可以较好地拟合波动性的分析和预测。    
A AR模型    
B MA模型    
C ARMA模型    
D GARCH模型        
解析：AR auto regressive model AR模型是一种线性预测
MA模型(moving average model)滑动平均模型，其中使用趋势移动平均法建立直线趋势的预测模型。
ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐。    
GARCH模型称为广义ARCH模型，是ARCH模型的拓展，由Bollerslev(1986)发展起来的。它是ARCH模型的推广。GARCH(p,0)模型，相当于ARCH(p)模型。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测。    
正确答案 D    
2、以下说法中错误的是（）    
A  SVM对噪声（如来自其他分部的噪声样本）具备鲁棒性    
B  在adaboost算法中，所有被分错样本的权重更新比例不相同     
C  boosting和bagging都是组合多个分类器投票的方法，二者都是根据单个分类器的正确率确定其权重   
D  给定n个数据点，如果其中一半用于训练，一半用户测试，则训练误差和测试误差之间的差别会随着n的增加而减少的    
解析：    
A   软间隔分类器对噪声是有鲁棒性的    
B   具体说来，整个Adaboost 迭代算法就3步：    
初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。
训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。    
将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。    
C    boosting是根据分类器正确率确定权重，bagging不是。    
Bagging即套袋法，其算法过程如下：    
A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）    
B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）   
C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）
Boosting其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。    
关于Boosting的两个核心问题：    
1）在每一轮如何改变训练数据的权值或概率分布？
通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。    
2）通过什么方式来组合弱分类器？    
通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。
而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。    
D    训练集变大会提高模型鲁棒性。    
正确答案C    
3、你正在使用带有 L1 正则化的 logistic 回归做二分类，其中 C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。当你把 C 值从 0 增加至非常大的值时，下面哪个选项是正确的？    
![image](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1514624605_280.png)
A 第一个 w2 成了 0，接着 w1 也成了 0    
B 第一个 w1 成了 0，接着 w2 也成了 0    
C w1 和 w2 同时成了 0    
D 即使在 C 成为大值之后，w1 和 w2 都不能成 0    
解析：L1正则化的函数如图，所以w1和w2可以为0。同时w1和w2是对称的，不会导致一个为0另一个不为0的状态。    
正确答案D    
4、在 k-均值算法中，以下哪个选项可用于获得全局最小？    
A 尝试为不同的质心（centroid）初始化运行算法    
B 整迭代的次数   
C 找到集群的最佳数量    
D 以上所有   
解析：所有都可以用来调试以找到全局最小。    
正确答案D    
5、假设你使用 log-loss 函数作为评估标准。下面这些选项，哪些是对作为评估标准的 log-loss 的正确解释。    
A 如果一个分类器对不正确的分类很自信，log-loss 会严重的批评它   
B 对一个特别的观察而言，分类器为正确的类别分配非常小的概率，然后对 log-loss 的相应分布会非常大    
C log-loss 越低，模型越好   
D 以上都是   
解析：损失函数总结(https://blog.csdn.net/ZHANG781068447/article/details/82752598)
正确答案D
6、下面哪个选项中哪一项属于确定性算法？
A  PCA   
B  K-Means   
C  以上都不是   
解析：确定性算法表明在不同运行中，算法输出并不会改变。如果我们再一次运行算法，PCA 会得出相同的结果，而 k-means 不会。    
正确答案：A    
7、两个变量的 Pearson 相关性系数为零，但这两个变量的值同样可以相关。这句描述是正确还是错误？   
A 正确   
B 错误    
解析：
答案为（A）：Pearson相关系数只能衡量线性相关性，但无法衡量非线性关系。如y=x^2，x和y有很强的非线性关系。

8、下面哪个/些超参数的增加可能会造成随机森林数据过拟合？        
A 树的数量    
B 树的深度    
C 学习速率    
解析：   
答案为（B）：通常情况下，我们增加树的深度有可能会造成模型过拟合。学习速率并不是随机森林的超参数。增加树的数量可能会造成欠拟合。   
9、下列哪个不属于常用的文本分类的特征选择算法？    
A 卡方检验值    
B 互信息    
C 信息增益    
D 主成分分析    
解析：    
答案D    
常采用特征选择方法。常见的六种特征选择方法：    
1）DF(Document Frequency) 文档频率             
DF:统计特征词出现的文档数量，用来衡量某个特征词的重要性    
2）MI(Mutual Information) 互信息法    
互信息法用于衡量特征词与文档类别直接的信息量。
如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向"低频"的特征词。    
相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。    
3）(Information Gain) 信息增益法    
通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。    
4）CHI(Chi-square) 卡方检验法    
利用了统计学中的"假设检验"的基本思想：首先假设特征词与类别直接是不相关的    
如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。    
5）WLLR(Weighted Log Likelihood Ration)加权对数似然    
6）WFO（Weighted Frequency and Odds）加权频率和可能性    
10、机器学习中做特征选择时，可能用到的方法有？    
A 卡方    
B 信息增益
C 平均互信息    
D 期望交叉熵    
E 以上都有    
正确答案是：E    


## 第五天    
1、下列方法中，不可以用于特征降维的方法包括   
A 主成分分析PCA    
B 线性判别分析LDA    
C 深度学习SparseAutoEncoder    
D 矩阵奇异值分解SVD    
正确答案是：C    
特征降维方法主要有：    
PCA，LLE，Isomap
SVD和PCA类似，也可以看成一种降维方法
LDA:线性判别分析，可用于降维    
AutoEncoder：AutoEncoder的结构与神经网络的隐含层相同，由输入L1,输出  L2组成，中间则是权重连接。Autoencoder通过L2得到输入的重构L3，最小化L3与L1的差别  进行训练得到权重。在这样的权重参数下，得到的L2可以尽可能的保存L1的信息。     
Autoencoder的输出L2的维度由输出的神经元个数决定。当输出维度大于L1时，则需要在训练目标函数中加入sparse    惩罚项，避免L2直接复制L1（权重全为1）。所以称为sparseAutoencoder( Andrew Ng提出的)。
结论：SparseAutoencoder大多数情况下都是升维的，所以称之为特征降维的方法不准确。
![image](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1514641291_629.png)    
2、下列哪些不特别适合用来对高维数据进行降维     
A LASSO     
B 主成分分析法     
C 聚类分析     
D 小波分析法        
E 线性判别法    
F 拉普拉斯特征映射       
正确答案是：C
lasso通过参数缩减达到降维的目的；    
pca就不用说了     
线性鉴别法即LDA通过找到一个空间使得类内距离最小类间距离最大所以可以看做是降维；    
小波分析有一些变换的操作降低其他干扰可以看做是降维     
拉普拉斯请看这个http://f.dataguru.cn/thread-287243-1-1.html     
3、下列属于无监督学习的是    
A k-means    
B SVM    
C 最大熵    
D CRF    
正确答案：A     
 A是聚类，属于无监督学习。BC是分类，属于监督学习。至于D是序列化标注，也是有监督学习。    
4、下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）    
A 特征灵活    
B 速度快    
C 可容纳较多上下文信息    
D 全局最优     
正确答案是： B    
CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢
CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） ————与HMM比较    
同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­————与MEMM比较    
CRF是在给定需要标记的观察序列的条件下，使用维特比算法，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。————与ME比较    
5、以下哪个是常见的时间序列算法模型    
A RSI    
B MACD    
C ARMA    
D KDJ    
正确答案是：C    
自回归滑动平均模型(ARMA)     
其建模思想可概括为：逐渐增加模型的阶数，拟合较高阶模型，直到再增加模型的阶数而剩余残差方差不再显著减小为止。    
其他三项都不是一个层次的。     
A.相对强弱指数 (RSI, Relative Strength Index)     是通过比较一段时期内的平均收盘涨数和平均收盘跌数来分析市场买沽盘的意向和实力 , 从而作出未来市场的走势 .    
B.移动平均聚散指标 (MACD, Moving Average Convergence Divergence),     是根据均线的构造原理 , 对股票价格的收盘价进行平滑处理 , 求出算术平均值以后再进行计算 , 是一种趋向类指标 .    
D. 随机指标 (KDJ) 一般是根据统计学的原理 , 通过一个特定的周期 ( 常为 9 日 ,9 周等 ) 内出现过的最高价 , 最低价及最后一个计算周期的收盘价及这三者之间的比例关系 , 来计算最后一个计算周期的未成熟随机值 RSV, 然后根据平滑移动平均线的方法来计算 K 值 , D 值与 J 值 , 并绘成曲线图来研判股票走势 .    
6、下列不是SVM核函数的是    
A 多项式核函数    
B logistic核函数    
C 径向基核函数    
D Sigmoid核函数        
正确答案是： B    
SVM核函数包括线性核函数、多项式核函数、径向基核函数、高斯核函数、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数。
核函数的定义并不困难，根据泛函的有关理论，只要一种函数 K ( x i , x j ) 满足Mercer条件，它就对应某一变换空间的内积．对于判断哪些函数是核函数到目前为止也取得了重要的突破，得到Mercer定理和以下常用的核函数类型：    
(1)线性核函数     
K ( x , x i ) = x ⋅ x i    
(2)多项式核     
K ( x , x i ) = ( ( x ⋅ x i ) + 1 ) d     
(3)径向基核（RBF）    
K ( x , x i ) = exp ( − ∥ x − x i ∥ 2 σ 2 )     
Gauss径向基函数则是局部性强的核函数，其外推能力随着参数 σ 的增大而减弱。多项式形式的核函数具有良好的全局性质。局部性较差。    
(4)傅里叶核     
K ( x , x i ) = 1 − q 2 2 ( 1 − 2 q cos ( x − x i ) + q 2 )    
(5)样条核 
K ( x , x i ) = B 2 n + 1 ( x − x i )    
(6)Sigmoid核函数     
K ( x , x i ) = tanh ( κ ( x , x i ) − δ )    
采用Sigmoid函数作为核函数时，支持向量机实现的就是一种多层感知器神经网络，应用SVM方法，隐含层节点数目(它确定神经网络的结构)、隐含层节点对输入节点的权值都是在设计(训练)的过程中自动确定的。而且支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最小值，也保证了它对于未知样本的良好泛化能力而不会出现过学习现象。

核函数的选择    
在选取核函数解决实际问题时，通常采用的方法有：    
一是利用专家的先验知识预先选定核函数；    
二是采用Cross-Validation方法，即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数．如针对傅立叶核、RBF核，结合信号处理问题中的函数回归问题，通过仿真实验，对比分析了在相同数据条件下，采用傅立叶核的SVM要比采用RBF核的SVM误差小很多．    
三是采用由Smits等人提出的混合核函数方法，该方法较之前两者是目前选取核函数的主流方法，也是关于如何构造核函数的又一开创性的工作．将不同的核函数结合起来后会有更好的特性，这是混合核函数方法的基本思想．    
7、解决隐马模型中预测问题的算法是    
A 前向算法    
B 后向算法    
C Baum-Welch算法    
D 维特比算法    
正确答案是：D    
A、B：前向、后向算法解决的是一个评估问题，即给定一个模型，求某特定观测序列的概率，用于评估该序列最匹配的模型。    
C：Baum-Welch算法解决的是一个模型训练问题，即参数估计，是一种无监督的训练方法，主要通过EM迭代实现；     
D：维特比算法解决的是给定      一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。如通过海藻变化（输出序列）来观测天气（状态序列），是预测问题，通信中的解码问题。    
58、一般，k-NN最近邻方法在（）的情况下效果较好    
A 样本较多但典型性不好    
B 样本较少但典型性好    
C 样本呈团状分布    
D 样本呈链状分布    
正确答案是： B    
K近邻算法主要依靠的是周围的点，因此如果样本过多，那肯定是区分不出来的。因此应当选择B    
样本呈团状颇有迷惑性，这里应该指的是整个样本都是呈团状分布，这样kNN就发挥不出其求近邻的优势了，整体样本应该具有典型性好，样本较少，比较适宜。    
9、在一个n维的空间中， 最好的检测outlier(离群点)的方法是（）    
A 作正态分布概率图    
B 作盒形图    
C 马氏距离    
D 作散点图       
正确答案是：C    
解析：
![](https://ws3.sinaimg.cn/large/006tNbRwly1fvelohkwanj30la09aacb.jpg)    
10、对数几率回归（logistics regression）和一般回归分析有什么区别？    
A 对数几率回归是设计用来预测事件可能性的    
B 对数几率回归可以用来度量模型拟合程度    
C 对数几率回归可以用来估计回归系数    
D 以上所有    
正确答案是：D     
解析：    
A: 对数几率回归其实是设计用来解决分类问题的    
B: 对数几率回归可以用来检验模型对数据的拟合度    
C: 虽然对数几率回归是用来解决分类问题的，但是模型建立好后，就可以根据独立的特征，估计相关的回归系数。就我认为，这只是估计回归系数，不能直接用来做回归模型。 

## 第六天
61、bootstrap数据是什么意思？（提示：考“bootstrap”和“boosting”区别）    
A 有放回地从总共M个特征中抽样m个特征    
B 无放回地从总共M个特征中抽样m个特征    
C 有放回地从总共N个样本中抽样n个样本    
D 无放回地从总共N个样本中抽样n个样本    
正确答案是：C    
bootstrap其实就是bagging的意思，是Bootstrap Aggregating的缩写，根据定义就可知是有放回的采样，    
从m个样本的原始数据集里进行n（n<=m）次采样，构成一个包含n个样本的新训练数据集，然后拿这个新的数据集来训练模型。重复上述过程B次，得到B个模型，当有新的模型需要进行预测时，拿这B个模型分别对这个样本进行预测，然后采用投票的方式（分类问题）或求平均值（回归问题）得到新样本的预测值。    
62、“过拟合”只在监督学习中出现，在非监督学习中，没有“过拟合”，这是（）    
A 对的    
B 错的    
正确答案是： B    
我们可以评估无监督学习方法通过无监督学习的指标    
63、对于k折交叉验证, 以下对k的说法正确的是（）    
A k越大, 不一定越好, 选择大的k会加大评估时间    
B 选择更大的k, 就会有更小的bias (因为训练集更加接近总数据集)    
C 在选择k时, 要最小化数据集之间的方差    
D 以上所有    
正确答案：D    
k越大, bias越小, 训练时间越长. 在训练时, 也要考虑数据集间方差差别不大的原则. 比如, 对于二类分类问题, 使用2-折交叉验证, 如果测试集里的数据都是A类的, 而训练集中数据都是B类的, 显然, 测试效果会很差.    
64、回归模型中存在多重共线性, 你如何解决这个问题？    
1 去除这两个共线性变量    
2 我们可以先去除一个共线性变量        
3 计算VIF(方差膨胀因子), 采取相应措施        
4 为了避免损失信息, 我们可以使用一些正则化方法, 比如, 岭回归和lasso回归       
A 1    
B 2    
C 2和3    
D 2, 3和4    
正确答案是：D    
解决多重公线性, 可以使用相关矩阵去去除相关性高于75%的变量 (有主观成分). 也可以VIF, 如果VIF值<=4说明相关性不是很高, VIF值>=10说明相关性较高.
我们也可以用 岭回归和lasso回归的带有惩罚正则项的方法. 我们也可以在一些变量上加随机噪声, 使得变量之间变得不同, 但是这个方法要小心使用, 可能会影响预测效果。    
65、模型的高bias是什么意思, 我们如何降低它 ？    
A 在特征空间中减少特征    
B 在特征空间中增加特征    
C 增加数据点    
D B和C    
E 以上所有    
正确答案是： B    
bias太高说明模型太简单了, 数据维数不够, 无法准确预测数据, 所以, 升维吧 !    
66、训练决策树模型, 属性节点的分裂, 具有最大信息增益的图是下图的哪一个（）    
![](https://ws1.sinaimg.cn/large/006tNbRwly1fvf7rrms94j30c909eq40.jpg)    
A Outlook    
B Humidity    
C Windy    
D Temperature    
正确答案是：A    
根据信息增益的定义计算可得。    
67、对于信息增益, 决策树分裂节点, 下面说法正确的是（）     
1 纯度高的节点需要更多的信息去区分    
2 信息增益可以用”1比特-熵”获得    
3 如果选择一个属性具有许多归类值, 那么这个信息增益是有偏差的    
A 1    
B 2    
C 2和3    
D 所有以上    
正确答案是：C    
纯度越高，表示不确定越少，更少的信息就可以区分    
68、下图是同一个SVM模型, 但是使用了不同的径向基核函数的gamma参数, 依次是g1, g2, g3 , 下面大小比较正确的是    
![](https://ws4.sinaimg.cn/large/006tNbRwly1fvf7w94cxej30gi07fjs0.jpg)    
A g1 > g2 > g3    
B g1 = g2 = g3    
C g1 < g2 < g3    
D g1 >= g2 >= g3E. g1 <= g2 <= g3    
正确答案是：C    
所谓径向基函数 (Radial Basis Function 简称 RBF), 就是某种沿径向对称的标量函数。 通常定义为空间中任一点x到某一中心点xc之间欧氏距离的单调函数 , 可记作 k(||x-xc||), 其作用往往是局部的 , 即当x远离xc时函数取值很小。最常用的径向基函数是高斯核函数 ,形式为 k(||x-xc||)=exp{- ||x-xc||^2/(2*σ^2) } 其中xc为核函数中心,σ为函数的宽度参数 , 控制了函数的径向作用范围。由radial basis: exp(-gamma*|u-v|^2)可知, gamma越小, 模型越简单, 平滑度越好, 分类边界越不容易过拟合, 所以选C。    
69、假设我们要解决一个二类分类问题, 我们已经建立好了模型, 输出是0或1, 初始时设阈值为0.5, 超过0.5概率估计, 就判别为1, 否则就判别为0 ; 如果我们现在用另一个大于0.5的阈值,  那么现在关于模型说法, 正确的是 :     
1 模型分类的召回率会降低或不变    
2 模型分类的召回率会升高    
3 模型分类准确率会升高或不变    
4 模型分类准确率会降低    
A 1    
B 2    
C 1和3    
D 2和4    
E 以上都不是    
正确答案是：A    
精确率, 准确率和召回率是广泛用于信息检索和统计学分类领域的度量值，用来评价结果的质量。下图可以帮助理解和记忆它们之间的关系, 其中精确率(precision)和准确率(accuracy)都是关于预测效果的描述. 召回率是关于预测样本的描述。
精确率表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP), 也就是P = TP / (TP + FP)。

准确率表示的是预测的正负样本有多少是真实的正和负, 预测正确的数量占全部预测数量的比例, 也就是A = (TP + TN) / (TP + FP + TN + FN) = (TP + TN) / 全部样本。

召回率表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN), 也就是R = TP / (TP + FN)。
精确率和召回率二者计算方法其实就是分母不同，一个分母是预测为正的样本数，另一个是原来样本中所有的正样本数。
提高分界阈值大于0.5, 则预测为正的样本数要降低, 相当于把图中圆圈变小, 按下图则可计算    
![](https://ws1.sinaimg.cn/large/006tNbRwly1fvf80ecoo7j30du0sbmyp.jpg)    
召回率的分子变小分母不变, 所以召回率会变小或不变;
精确率的分子分母同步变化, 所以精确率的变化不能确定;
准确率的分子为圆内绿色加圆外右侧矩形面积所围样本, 两者之和变化不能确定; 分母为矩形所含全部样本不变化, 所以准确率的变化不能确定;
综上, 所以选A。    
70、“点击率问题”是这样一个预测问题, 99%的人是不会点击的, 而1%的人是会点击进去的, 所以这是一个非常不平衡的数据集. 假设, 现在我们已经建了一个模型来分类, 而且有了99%的预测准确率, 我们可以下的结论是        
A 模型预测准确率已经很高了, 我们不需要做什么了    
B 模型预测准确率不高, 我们需要做点什么改进模型    
C 无法下结论    
D 以上都不对    
正确答案是：C    
类别不均衡的情况下，不要用准确率做分类评估指标，因为全判断为不会点，准确率也是99%，但是这个分类器一点用都没有。
此时应该用查准率或查全率，更加能反映情况。    

## 第七天

1、使用k=1的knn算法, 下图二类分类问题, “+” 和 “o” 分别代表两个类, 那么, 用仅拿出一个测试样本的交叉验证方法, 交叉验证的错误率是多少：    
![](https://ws4.sinaimg.cn/large/006tNbRwly1fvg8u824ckj30lc0gwjrv.jpg)
A
0%    
B
100%    
C
0%到100    
D
以上都不是    
正确答案是： B    
knn算法就是, 在样本周围看k个样本, 其中大多数样本的分类是A类, 我们就把这个样本分成A类. 显然, k=1 的knn在上图不是一个好选择, 分类的错误率始终是100%。

2、我们想在大数据集上训练决策树, 为了使用较少时间, 我们可以    
A
增加树的深度    
B
增加学习率 (learning rate)    
C
减少树的深度    
D
减少树的数量    
正确答案是：C    
增加树的深度, 会导致所有节点不断分裂, 直到叶子节点是纯的为止. 所以, 增加深度, 会延长训练时间.
决策树没有学习率参数可以调. (不像集成学习和其它有步长的学习方法)
决策树只有一棵树, 不是随机森林。    
3、假如我们使用非线性可分的SVM目标函数作为最优化对象, 我们怎么保证模型线性可分？    
A
设C=1    
B
设C=0    
C
设C=无穷大    
D
以上都不对    
正确答案是：C    
C无穷大保证了所有的线性不可分都是可以忍受的.    
4、以下哪些算法, 可以用神经网络去构造:     
1. KNN    
2. 线性回归    
3. 对数几率回归

A 1和 2    
B 2 和 3    
C 1, 2 和 3    
D 以上都不是    
 正确答案是： B    
1. KNN算法不需要训练参数, 而所有神经网络都需要训练参数, 因此神经网络帮不上忙
2. 最简单的神经网络, 感知器, 其实就是线性回归的训练
3. 我们可以用一层的神经网络构造对数几率回归

5、请选择下面可以应用隐马尔科夫(HMM)模型的选项    
A
基因序列数据集    
B
电影浏览数据集    
C
股票市场数据集    
D
所有以上    
正确答案是：D    
只要是和时间序列问题有关的 , 都可以试试HMM    
6、我们建立一个5000个特征, 100万数据的机器学习模型. 我们怎么有效地应对这样的大数据训练 :     
A
我们随机抽取一些样本, 在这些少量样本之上训练    
B
我们可以试用在线机器学习算法    
C
我们应用PCA算法降维, 减少特征数    
D
B 和 C    
E
A 和 B    
F
以上所有    
正确答案是：F    
样本数过多, 或者特征数过多, 而不能单机完成训练, 可以用小批量样本训练, 或者在线累计式训练, 或者主成分PCA降维方式减少特征数量再进行训练.     

7、我们想要减少数据集中的特征数, 即降维. 选择以下适合的方案 :    
1. 使用前向特征选择方法
2. 使用后向特征排除方法
3. 我们先把所有特征都使用, 去训练一个模型, 得到测试集上的表现. 然后我们去掉一个特征, 再去训练, 用交叉验证看看测试集上的表现. 如果表现比原来还要好, 我们可以去除这个特征.
4. 查看相关性表, 去除相关性最高的一些特征    
A
1 和 2    
B
2, 3和4    
C
1, 2和4    
D
All    
正确答案是：D    
1.前向特征选择方法和后向特征排除方法是我们特征选择的常用方法    
2.如果前向特征选择方法和后向特征排除方法在大数据上不适用, 可以用这里第三种方法.     
3.用相关性的度量去删除多余特征, 也是一个好方法

8、对于随机森林和GradientBoosting Trees, 下面说法正确的是:    
1 在随机森林的单个树中, 树和树之间是有依赖的, 而GradientBoosting     Trees中的单个树之间是没有依赖的
2 这两个模型都使用随机特征子集, 来生成许多单个的树    
3 我们可以并行地生成GradientBoosting Trees单个树, 因为它们之间是没有依赖的    
4 GradientBoosting Trees训练模型的表现总是比随机森林好    
A
2    
B
1 and 2    
C
1, 3 and 4    
D
2 and 4    
正确答案是：A    
1 随机森林是基于bagging的, 在随机森林的单个树中, 树和树之间是没有依赖的。
2 Gradient Boosting trees是基于boosting的，且GradientBoosting Trees中的单个树之间是有依赖关系。
3 这两个模型都使用随机特征子集, 来生成许多单个的树。
所以题干中只有第二点是正确的，选A。    
9、对于PCA(主成分分析)转化过的特征 ,  朴素贝叶斯的”不依赖假设”总是成立, 因为所有主要成分是正交的, 这个说法是 :    

A
正确的    
B
错误的    
正确答案是： B    
这个说法是错误的, 首先, “不依赖”和”不相关”是两回事, 其次, 转化过的特征, 也可能是相关的    
10、对于PCA说法正确的是 :    
1. 我们必须在使用PCA前规范化数据
2. 我们应该选择使得模型有最大variance的主成分
3. 我们应该选择使得模型有最小variance的主成分
4. 我们可以使用PCA在低维度上做数据可视化    
A
1, 2 and 4    
B
2 and 4    
C
3 and 4    
D
1 and 3    
E
1, 3 and 4    
正确答案是：A    
1）PCA对数据尺度很敏感, 打个比方, 如果单位是从km变为cm, 这样的数据尺度对PCA最后的结果可能很有影响(从不怎么重要的成分变为很重要的成分).    
2）我们总是应该选择使得模型有最大variance的主成分    
3）有时在低维度上左图是需要PCA的降维帮助的    


## 第八天
1、对于下图, 最好的主成分选择是多少 ?     
![](https://ws2.sinaimg.cn/large/006tNbRwly1fvm1gjny11j30im0b640f.jpg)    
A
7    
B
30    
C
35    
D
Can’t Say    
正确答案是： B    
主成分选择使variance越大越好， 在这个前提下， 主成分越少越好。    
2、数据科学家可能会同时使用多个算法（模型）进行预测， 并且最后把这些算法的结果集成起来进行最后的预测（集成学习），以下对集成学习说法正确的是     
A
单个模型之间有高相关性    
B
单个模型之间有低相关性    
C
在集成学习中使用“平均权重”而不是“投票”会比较好    
D
单个模型都是用的一个算法    
正确答案是： B  ？？？？  
？？？bagging具有低相关性，而boosting是不是高相关性？？？


3、在有监督学习中， 我们如何使用聚类方法？     
1. 我们可以先创建聚类类别， 然后在每个类别上用监督学习分别进行学习
2. 我们可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习
3. 在进行监督学习之前， 我们不能新建聚类类别
4. 我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习    
A
2 和 4    
B
1 和 2    
C
3 和 4    
D
 1 和 3    
正确答案是： B    
我们可以为每个聚类构建不同的模型， 提高预测准确率。
“类别id”作为一个特征项去训练， 可以有效地总结了数据特征。    

4、以下说法正确的是 
1. 一个机器学习模型，如果有较高准确率，总是说明这个分类器是好的
2. 如果增加模型复杂度， 那么模型的测试错误率总是会降低
3. 如果增加模型复杂度， 那么模型的训练错误率总是会降低
4. 我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习    
A
1    
B
2    
C
3    
D
2和3    
E
都错    
正确答案是：E    
1的模型中, 如果负样本占比非常大,也会有很高的准确率, 对正样本的分类不一定很好;
4的模型中, “类别id”可以作为一个特征项去训练, 这样会有效地总结了数据特征。    

5、对应GradientBoosting tree算法， 以下说法正确的是:
1. 当增加最小样本分裂个数，我们可以抵制过拟合
2. 当增加最小样本分裂个数，会导致过拟合
3. 当我们减少训练单个学习器的样本个数，我们可以降低variance
4. 当我们减少训练单个学习器的样本个数，我们可以降低bias    
A
2 和 4    
B 
2 和 3    
C
1 和 3    
D
1 和 4    
正确答案是：C    
最小样本分裂个数是用来控制“过拟合”参数。太高的值会导致“欠拟合”，这个参数应该用交叉验证来调节。
第二点是靠bias和variance概念的。    

6、以下哪个图是KNN算法的训练边界    
![](https://ws4.sinaimg.cn/large/006tNbRwly1fvm1ppcfcoj30ik04y42q.jpg)     

A
B    
B
A    
C
D    
D
C    
E
都不是    
正确答案是： B     
KNN算法肯定不是线性的边界， 所以直的边界就不用考虑了。另外这个算法是看周围最近的k个样本的分类用以确定分类，所以边界一定是坑坑洼洼的。    



7、如果一个训练好的模型在测试集上有100%的准确率，     这是不是意味着在一个新的数据集上，也会有同样好的表现？    
A
是的，这说明这个模型的范化能力已经足以支持新的数据集合了    
B
不对，依然后其他因素模型没有考虑到，比如噪音数据       
正确答案是： B    
没有一个模型是可以总是适应新数据的。我们不可能可到100%准确率。    
8、下面的交叉验证方法    
i. 有放回的Bootstrap方法    
ii. 留一个测试样本的交叉验证    
iii. 5折交叉验证    
iv. 重复两次的5折教程验证    
当样本是1000时，下面执行时间的顺序，正确的是    
A
i > ii > iii > iv    
B
ii > iv > iii > i    
C
iv > i > ii > iii    
D
ii > iii > iv > i      
正确答案是： B    
Boostrap方法是传统地随机抽样，验证一次的验证方法，只需要训练1次模型，所以时间最少。    
留一个测试样本的交叉验证，需要n次训练过程（n是样本个数），这里，要训练1000个模型。    
5折交叉验证需要训练5个模型。    
重复2次的5折交叉验证，需要训练10个模型。    
所有B是正确的    
9、变量选择是用来选择最好的判别器子集， 如果要考虑模型效率，我们应该做哪些变量选择的考虑？
1. 多个变量其实有相同的用处
2. 变量对于模型的解释有多大作用
3. 特征携带的信息
4. 交叉验证    
A
1 和 4    
B
1, 2 和 3    
C
1,3 和 4    
D
以上所有    
正确答案是：C    
注意， 这题的题眼是考虑模型效率，所以不要考虑选项2.    


10、对于线性回归模型，包括附加变量在内，以下的可能正确的是 :
1. R-Squared 和 Adjusted R-squared都是递增的
2. R-Squared 是常量的，Adjusted R-squared是递增的
3. R-Squared 是递减的， Adjusted R-squared 也是递减的
4. R-Squared 是递减的， Adjusted R-squared是递增的    
A
1 和 2    
B
1 和 3    
C
2 和 4    
D
以上都不是    
正确答案是：D    
R-squared不能决定系数估计和预测偏差，这就是为什么我们要估计残差图。但是，R-squared有R-squared 和 predicted R-squared 所没有的问题。
每次你为模型加入预测器，R-squared递增或不变.    


## 第九天

1、对于下面三个模型的训练情况， 下面说法正确的是:  
![](https://ws2.sinaimg.cn/large/006tNc79ly1fvn4ksqstlj30io060dhh.jpg)    
1. 第一张图的训练错误与其余两张图相比，是最大的
2. 最后一张图的训练效果最好，因为训练错误最小
3. 第二张图比第一和第三张图鲁棒性更强，是三个里面表现最好的模型
4. 第三张图相对前两张图过拟合了
5. 三个图表现一样，因为我们还没有测试数据集   

A 1 和 3    
B 1 和 3    
C 1, 3 和 4    
D 5    
正确答案是：C    
最后一张过拟合, 训练错误最小, 第一张相反, 训练错误就是最大了. 所以1是对的;
仅仅训练错误最小往往说明过拟合, 所以2错, 4对;
第二张图平衡了拟合和过拟合, 所以3对;    
2、对于线性回归，我们应该有以下哪些假设？     
1. 找到离群点很重要, 因为线性回归对离群点很敏感
2. 线性回归要求所有变量必须符合正态分布
3. 线性回归假设数据没有多重线性相关性    
A
1 和 2    
B
2 和 3    
C
1,2 和 3    
D
以上都不是
正确答案是：D    
第1个假设, 离群点要着重考虑, 第一点是对的
第2个假设, 正态分布不是必须的. 当然, 如果是正态分布, 训练效果会更好
第3个假设, 有少量的多重线性相关性也是可以的, 但是我们要尽量避免    

3、当我们构造线性模型时, 我们注意变量间的相关性. 在相关矩阵中搜索相关系数时, 如果我们发现3对变量的相关系数是(Var1 和Var2, Var2和Var3, Var3和Var1)是-0.98, 0.45, 1.23 . 我们可以得出什么结论: 
1. Var1和Var2是非常相关的
2. 因为Var1和Var2是非常相关的, 我们可以去除其中一个
3. Var3和Var1的1.23相关系数是不可能的    

A
1 and 3    
B
1 and 2    
C
1,2 and 3    
D
1    
正确答案是：C    
相关性系数范围应该是 [-1,1]
一般地, 如果相关系数大于0.7或者小于-0.7, 是高相关的.
Var1和Var2相关系数是接近负1, 所以这是多重线性相关, 我们可以考虑去除其中一个. 
所以1, 2, 3个结论都是对的, 选C.     
4、如果在一个高度非线性并且复杂的一些变量中, 一个树模型可能比一般的回归模型效果更好. 这是（）    
A
对的    
B
错的    
正确答案是：A    
5、下面对集成学习模型中的弱学习者描述错误的是？    
A
他们经常不会过拟合    
B
他们通常带有高偏差，所以其并不能解决复杂学习问题    
C
他们通常会过拟合    
正确答案是：C    
注意是错误的描述    
弱学习者是问题的特定部分。所以他们通常不会过拟合，这也就意味着弱学习者通常拥有低方差和高偏差。    
6、下面哪个/些选项对 K 折交叉验证的描述是正确的？    
1.增大 K 将导致交叉验证结果时需要更多的时间    
2.更大的 K 值相比于小 K 值将对交叉验证结构有更高的信心    
3.如果 K=N，那么其称为留一交叉验证，其中 N 为验证集中的样本数量    
A
1 和 2    
B
2 和 3    
C
1 和 3    
D
1、2 和 3    
正确答案是：D    
大 K 值意味着对过高估计真实预期误差（训练的折数将更接近于整个验证集样本数）拥有更小的偏差和更多的运行时间（并随着越来越接近极限情况：留一交叉验证）。我们同样在选择 K 值时需要考虑 K 折准确度和方差间的均衡。    

7、最出名的降维算法是 PCA 和 t-SNE。将这两个算法分别应用到数据「X」上，并得到数据集「X_projected_PCA」，「X_projected_tSNE」。下面哪一项对「X_projected_PCA」和「X_projected_tSNE」的描述是正确的？    
A
X_projected_PCA 在最近邻空间能得到解释    
B
X_projected_tSNE 在最近邻空间能得到解释    
C
两个都在最近邻空间能得到解释    
D
两个都不能在最近邻空间得到解释    
正确答案是： B    
t-SNE 算法考虑最近邻点而减少数据维度。所以在使用 t-SNE 之后，所降的维可以在最近邻空间得到解释。但 PCA 不能。    



8、给定三个变量 X，Y，Z。(X, Y)、(Y, Z) 和 (X, Z) 的 Pearson 相关性系数分别为 C1、C2 和 C3。现在 X 的所有值加 2（即 X+2），Y 的全部值减 2（即 Y-2），Z 保持不变。那么运算之后的 (X, Y)、(Y, Z) 和 (X, Z) 相关性系数分别为 D1、D2 和 D3。现在试问 D1、D2、D3 和 C1、C2、C3 之间的关系是什么？    
A D1= C1, D2 < C2, D3 > C3    
B
D1 = C1, D2 > C2, D3 > C3    
C
D1 = C1, D2 > C2, D3 < C3    
D
D1 = C1, D2 < C2, D3 < C3    
E
D1 = C1, D2 = C2, D3 = C3        
正确答案是：E    
特征之间的相关性系数不会因为特征加或减去一个数而改变。    


9、为了得到和 SVD 一样的投射（projection），你需要在 PCA 中怎样做？    
A
将数据转换成零均值    
B
将数据转换成零中位数    
C
无法做到    
正确答案是：A    
当数据有一个 0 均值向量时，PCA 有与 SVD 一样的投射，否则在使用 SVD 之前，你必须将数据均值归 0    

10、假设我们有一个数据集，在一个深度为 6 的决策树的帮助下，它可以使用 100% 的精确度被训练。现在考虑一下两点，并基于这两点选择正确的选项。    
注意：所有其他超参数是相同的，所有其他因子不受影响。    
1.深度为 4 时将有高偏差和低方差    
2.深度为 4 时将有低偏差和低方差    
A
只有 1    
B
只有 2    
C
1 和 2    
D
没有一个    
正确答案是：A    
如果在这样的数据中你拟合深度为 4 的决策树，这意味着其更有可能与数据欠拟合。因此，在欠拟合的情况下，你将获得高偏差和低方差。    

## 第十天
1、在以下不同的场景中,使用的分析方法不正确的有    
A
根据商家最近一年的经营及服务数据,用聚类算法判断出天猫商家在各自主营类目下所属的商家层级    
B
根据商家近几年的成交数据,用聚类算法拟合出用户未来一个月可能的消费金额公式    
C
用关联规则算法分析出购买了汽车坐垫的买家,是否适合推荐汽车脚垫    
D
根据用户最近购买的商品信息,用决策树算法识别出淘宝买家可能是男还是女    
正确答案是： B    
预测消费更合适的算法是用回归模型来做。而不是聚类算法。    
2、以下对k-means聚类算法解释正确的是    
A
能自动识别类的个数,随即挑选初始点为中心点计算    
B
能自动识别类的个数,不是随即挑选初始点为中心点计算    
C
不能自动识别类的个数,随即挑选初始点为中心点计算    
D
不能自动识别类的个数,不是随即挑选初始点为中心点计算    
正确答案是：C    
（1）适当选择c个类的初始中心；    
（2）在第k次迭代中，对任意一个样本，求其到c个中心的距离，将该样本归到距离最短的中心所在的类；    
（3）利用均值等方法更新该类的中心值；    
（4）对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，否则继续迭代。
以上是KMeans（C均值）算法的具体步骤，可以看出需要选择类别数量，但初次选择是随机的，最终的聚类中心是不断迭代稳定以后的聚类中心。所以答案选C。    
3、（假设precision=TP/(TP+FP),recall=TP/(TP+FN)。）在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（）    
A
Accuracy:(TP+TN)/all    
B
F-value:2*recall*precision/(recall+precision)    
C
G-mean:sqrt(precision*recall)    
D
AUC:曲线下面积    
正确答案是：A    
题目提到测试集正例和负例数量不均衡，那么假设正例数量很少占10%，负例数量占大部分90%。
而且算法能正确识别所有负例，但正例只有一半能正确判别。
那么TP=0.05×all,TN=0.9×all，Accuracy=95%。
虽然Accuracy很高，precision是100%,但正例recall只有50%    
4、下列选项中,识别模式与其他不⼀样的是    
A
⽤户年龄分布判断:少年、青年、中年、⽼年    
B
医⽣给病⼈诊断发病类型    
C
投递员分拣信件    
D
消费者类型判断:⾼消费、⼀般消息、低消费    
E
出⾏方式判断:步⾏、骑车、坐车    
F
商家对商品分级    
正确答案是：E    
E属于预测问题，其他的选项属于分类问题    
5、在大规模的语料中，挖掘词的相关性是一个重要的问题。以下哪一个信息不能用于确定两个词的相关性。    
A
互信息    
B
最大熵    
C
卡方检验    
D
最大似然比    
正确答案是： B    
最大熵代表了整体分布的信息，通常具有最大熵的分布作为该随机变量的分布，不能体现两个词的相关性，但是卡方是检验两类事务发生的相关性。所以选B【正解】    
6、基于统计的分词方法为（）    
A
正向最大匹配法    
B
逆向最大匹配法    
C
最少切分    
D
条件随机场    
正确答案是：D    
第一类是基于语法和规则的分词法。其基本思想就是在分词的同时进行句法、语义分析,利用句法信息和语义信息来进行词性标注,以解决分词歧义现象。因为现有的语法知识、句法规则十分笼统、复杂,基于语法和规则的分词法所能达到的精确度远远还不能令人满意,目前这种分词系统还处在试验阶段。

第二类是机械式分词法（即基于词典）。机械分词的原理是将文档中的字符串与词典中的词条进行逐一匹配,如果词典中找到某个字符串,则匹配成功,可以切分,否则不予切分。基于词典的机械分词法,实现简单,实用性强,但机械分词法的最大的缺点就是词典的完备性不能得到保证。据统计,用一个含有70000个词的词典去切分含有15000个词的语料库,仍然有30%以上的词条没有被分出来,也就是说有4500个词没有在词典中登录。

第三类是基于统计的方法。基于统计的分词法的基本原理是根据字符串在语料库中出现的统计频率来决定其是否构成词。词是字的组合,相邻的字同时出现的次数越多,就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映它们成为词的可信度。

7、在下面的图像中，哪一个是多元共线（multi-collinear）特征？    
![](https://ws2.sinaimg.cn/large/006tNc79ly1fvocuy3mwtj30ki06z403.jpg)    
A
图 1 中的特征    
B
图 2 中的特征    
C
图 3 中的特征    
D
图 1、2 中的特征    
E
图 2、3 中的特征    
F
图 1、3 中的特征    
正确答案是：D    
在图 1 中，特征之间有高度正相关，图 2 中特征有高度负相关。所以这两个图的特征是多元共线特征。    
8、线性回归的基本假设不包括哪个？    
A
随机误差项是一个期望值为0的随机变量    
B
对于解释变量的所有观测值，随机误差项有相同的方差    
C
随机误差项彼此相关    
D
解释变量是确定性变量不是随机变量，与随机误差项之间相互独立    
E
随机误差项服从正态分布    
正确答案是：C    
9、下面哪些对「类型 1（Type-1）」和「类型 2（Type-2）」错误的描述是错误的？    
A
类型 1 通常称之为假正类，类型 2 通常称之为假负类    
B
类型 2 通常称之为假正类，类型 1 通常称之为假负类    
C
类型 1 错误通常在其是正确的情况下拒绝假设而出现    
正确答案是： B    
在统计学假设测试中，I 类错误即错误地拒绝了正确的假设即假正类错误，II 类错误通常指错误地接受了错误的假设即假负类错误。    
10、给线性回归模型添加一个不重要的特征可能会造成？    
A
增加 R-square    
B
减少 R-square    
正确答案是：A    
![](https://ws1.sinaimg.cn/large/006tNc79ly1fvodek9i7zj30ph0kz7a1.jpg)    


## 第十一天 
1、关于 ARMA 、 AR 、 MA 模型的功率谱，下列说法正确的是（ ）    
A
MA模型是同一个全通滤波器产生的    
B
MA模型在极点接近单位圆时，MA谱是一个深谷    
C
AR模型在零点接近单位圆时，AR谱是一个尖峰    
D
RMA谱既有尖峰又有深谷    
正确答案是：D    
MA模型滑动平均模型，模型参量法谱分析方法之一，也是现代谱估中常用的模型。
用MA模型法求信号谱估计的具体作法是：①选择MA模型，在输入是冲激函数或白噪声情况下，使其输出等于所研究的信号，至少应是对该信号一个好的近似。②利用已知的自相关函数或数据求MA模型的参数。③利用求出的模型参数估计该信号的功率谱。
AR 模型(auto regressive model)自回归模型，模型参量法高分辨率谱分析方法之一，也是现代谱估计中常用的模型。
用AR模型法求信具体作法是：
①选择AR模型，在输入是冲激函数或白噪声的情况下，使其输出等于所研究的信号，至少，应是对该信号的一个好的近似。
②利用已知的自相关函数或数据求模型的参数。
③利用求出的模型参数估计该信号的功率谱。
ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。    
2、符号集 a 、 b 、 c 、 d ，它们相互独立，相应概率为 1/2 、 1/4 、 1/8/ 、 1/16 ，其中包含信息量最小的符号是（ ）    
A
a    
B
b    
C
c    
D
d      
正确答案是：A    
因为消息出现的概率越小，则消息中所包含的信息量就越大。因此选a,同理d信息量最大。    
3、下列哪个不属于常用的文本分类的特征选择算法？    
A
卡方检验值    
B
互信息    
C
信息增益    
D
主成分分析    
正确答案是：D     
主成分分析是特征转换算法（特征抽取），而不是特征选择    
4、在数据清理中，下面哪个不是处理缺失值的方法?    


A
估算    
B 
整例删除    
C
变量删除    
D
成对删除    
正确答案是：D    
数据清理中，处理缺失值的方法有两种：
一、删除法：
1）删除观察样本
2）删除变量：当某个变量缺失值较多且对研究目标影响不大时，可以将整个变量整体删除
3）使用完整原始数据分析：当数据存在较多缺失而其原始数据完整时，可以使用原始数据替代现有数据进行分析
4）改变权重：当删除缺失数据会改变数据结构时，通过对完整数据按照不同的权重进行加权，可以降低删除缺失数据带来的偏差
二、查补法：均值插补、回归插补、抽样填补等    
5、统计模式分类问题中，当先验概率未知时，可以使用（）    
A
最小最大损失准则    
B
最小误判概率准则    
C
最小损失准则    
D
N-P判决    
正确答案是：A    
A. 考虑p(wi)变化的条件下，是风险最小 

B. 最小误判概率准则， 就是判断p(w1|x)和p(w2|x)哪个大，x为特征向量，w1和w2为两分类，根据贝叶斯公式，需要用到先验知识 

C. 最小损失准则，在B的基础之上，还要求出p(w1|x)和p(w2|x)的期望损失，因为B需要先验概率，所以C也需要先验概率 

D. N-P判决，即限定一类错误率条件下使另一类错误率为最小的两类别决策，即在一类错误率固定的条件下，求另一类错误率的极小值的问题，直接计算p(x|w1)和p(x|w2)的比值，不需要用到贝叶斯公式_

6、决策树的父节点和子节点的熵的大小关系是什么？    
A
A. 决策树的父节点更大    
B
B. 子节点的熵更大    
C
C. 两者相等    
D
D. 根据具体情况而定    
正确答案是：D    
假设一个父节点有2正3负样本，进一步分裂情况1：两个叶节点（2正，3负）；情况2：两个叶节点（1正1负，1正2负）。分别看下情况1和情况2，分裂前后确实都有信息增益，但是两种情况里不是每一个叶节点都比父节点的熵小。

7、语言模型的参数估计经常使用MLE（最大似然估计）。面临的一个问题是没有出现的项概率为0，这样会导致语言模型的效果不好。为了解决这个问题，需要使用（ ）    
A
平滑    
B
去噪    
C
随机插值    
D
增加白噪音    
正确答案是：A    
A，拉普拉斯平滑假设，将分子和分母各加上一个常数项。    
8、逻辑回归与多元回归分析有哪些不同？    
A
逻辑回归预测某事件发生的概率    
B
逻辑回归有较高的拟合效果    
C
逻辑回归回归系数的评估    
D
以上全选    
正确答案是：D    
逻辑回归是用于分类问题，我们能计算出一个事件/样本的概率；一般来说，逻辑回归对测试数据有着较好的拟合效果；建立逻辑回归模型后，我们可以观察回归系数类标签(正类和负类)与独立变量的的关系。    
9、"过拟合是有监督学习的挑战，而不是无监督学习"以上说法是否正确：    
A
正确    
B
错误    
正确答案是： B    
我们可以评估无监督学习方法通过无监督学习的指标，如：我们可以评估聚类模型通过调整兰德系数。    

10、中文同义词替换时，常用到Word2Vec，以下说法错误的是    
A
Word2Vec基于概率统计    
B
Word2Vec结果符合当前预料环境    
C
Word2Vec得到的都是语义上的同义词    
D
Word2Vec受限于训练语料的数量和质量    
正确答案是：C    
Word2vec，为一群用来产生词向量的相关模型。这些模型为浅而双层的神经网络，用来训练以重新建构语言学之词文本。网络以词表现，并且需猜测相邻位置的输入词，在word2vec中词袋模型假设下，词的顺序是不重要的。

训练完成之后，word2vec模型可用来映射每个词到一个向量，可用来表示词对词之间的关系。该向量为神经网络之隐藏。
Word2vec依赖skip-grams或连续词袋（CBOW）来建立神经词嵌入。    

## 第十二天
1、假定你用一个线性SVM分类器求解二类分类问题，如下图所示，这些用红色圆圈起来的点表示支持向量
![](https://ws1.sinaimg.cn/large/006tNc79ly1fvqn71tarwj30b306zdfx.jpg)    
如果移除这些圈起来的数据，决策边界（即分离超平面）是否会发生改变？    
A
Yes      
B
No    
正确答案是： B    
从数据的分布来看，移除那三个数据，决策边界不会受影响。    
2、如果将数据中除圈起来的三个点以外的其他数据全部移除，那么决策边界是否会改变？
![image](http://note.youdao.com/yws/res/4017/51DFE642FD6042459AF35AD57D8BAAC3)    
A
会    
B
不会    
正确答案是： B    
决策边界只会被支持向量影响，跟其他点无关。    
3、关于SVM泛化误差描述正确的是    
A
超平面与支持向量之间距离    
B
SVM对未知数据的预测能力    
C
SVM的误差阈值    
正确答案是： B    
统计学中的泛化误差是指对模型对未知数据的预测能力。    
4、以下关于硬间隔hard margin描述正确的是    
A
SVM允许分类存在微小误差    
B
SVM允许分类是有大量误差    
正确答案是：A    
硬间隔意味着SVM在分类时很严格，在训练集上表现尽可能好，有可能会造成过拟合。    
5、训练SVM的最小时间复杂度为O(n2)，那么一下哪种数据集不适合用SVM?    
A
大数据集    
B
小数据集    
C
中等大小数据集    
D
和数据集大小无关      
正确答案是：A    
有明确分类边界的数据集最适合SVM    
6、SVM的效率依赖于    
A
核函数的选择    
B
核参数    
C
软间隔参数    
D
以上所有    
正确答案是：D    
SVM的效率依赖于以上三个基本要求，它能够提高效率，降低误差和过拟合    
7、支持向量是那些最接近决策平面的数据点    
A
对    
B
错    
正确答案是：A    
支持向量就在间隔边界上    ？？？？
软间隔会不成立吧，分类错误的更近。    
8、SVM在下列那种情况下表现糟糕    
A
线性可分数据    
B
清洗过的数据    
C
含噪声数据与重叠数据点    
正确答案是：C    
当数据中含有噪声数据与重叠的点时，要画出干净利落且无误分类的超平面很难    
9、假定你使用了一个很大γ值的RBF核，这意味着：    
A
模型将考虑使用远离超平面的点建模    
B
模型仅使用接近超平面的点来建模    
C
模型不会被点到超平面的距离所影响    
D
以上都不正确    
正确答案是： B    
SVM调参中的γ衡量距离超平面远近的点的影响。
对于较小的γ，模型受到严格约束，会考虑训练集中的所有点，而没有真正获取到数据的模式、对于较大的γ，模型能很好地学习到模型。    
10、SVM中的代价参数表示：    
A
交叉验证的次数   
B
使用的核   
C
误分类与模型复杂性之间的平衡    
D
以上均不是    
正确答案是：C    
代价参数决定着SVM能够在多大程度上适配训练数据。
如果你想要一个平稳的决策平面，代价会比较低；如果你要将更多的数据正确分类，代价会比较高。可以简单的理解为误分类的代价。


## 第十三天
1、假定你使用SVM学习数据X，数据X里面有些点存在错误。现在如果你使用一个二次核函数，多项式阶数为2，使用松弛变量C作为超参之一。
当你使用较大的C（C趋于无穷），则：    
A
仍然能正确分类数据    
B
不能正确分类    
C
不确定    
D
以上均不正确    
正确答案是：A    
采用更大的C，误分类点的惩罚就更大，因此决策边界将尽可能完美地分类数据。    
2、假定你使用SVM学习数据X，数据X里面有些点存在错误。现在如果你使用一个二次核函数，多项式阶数为2，使用松弛变量C作为超参之一。
如果使用较小的C（C趋于0），则：    
A
误分类    
B
正确分类    
C
不确定    
D
以上均不正确    
正确答案是：A    
分类器会最大化大多数点之间的间隔，少数点会误分类，因为惩罚太小了。
3、如果我使用数据集的全部特征并且能够达到100%的准确率，但在测试集上仅能达到70%左右，这说明：    
A
欠拟合    
B
模型很棒    
C
过拟合    
正确答案是：C    

4、下面哪个属于SVM应用    
A
文本和超文本分类    
B
图像分类    
C
新文章聚类    
D
以上均是    
正确答案是：D    
SVM广泛应用于实际问题中，包括回归，聚类，手写数字识别等。    
5、假设你训练SVM后，得到一个线性决策边界，你认为该模型欠拟合。在下次迭代训练模型时，应该考虑：    
A
增加训练数据    
B
减少训练数据    
C
计算更多变量    
D
减少特征    
正确答案是：C    
由于是欠拟合，最好的选择是创造更多特征带入模型训练。    
6、假设你训练SVM后，得到一个线性决策边界，你认为该模型欠拟合。假如你想修改SVM的参数，同样达到模型不会欠拟合的效果，应该怎么做？    
A
增大参数C    
B
减小参数C    
C
改变C并不起作用    
D
以上均不正确    
正确答案是：A    
增大参数C会得到正则化模型
    
7、SVM中使用高斯核函数之前通常会进行特征归一化，以下关于特征归一化描述不正确的是？    
A
经过特征正则化得到的新特征优于旧特征    
B
特征归一化无法处理类别变量    
C
SVM中使用高斯核函数时，特征归一化总是有用的    
正确答案是：C   
不一定有用
8、假设现在只有两个类，这种情况下SVM需要训练几次？    
A
1    
B
2    
C
3    
D
4    
正确答案是：A    
两个类训练1次就可以了    
9、假设你训练了一个基于线性核的SVM，多项式阶数为2，在训练集和测试集上准确率都为100%。
如果增加模型复杂度或核函数的多项式阶数，将会发生什么？    
A
导致过拟合    
B
导致欠拟合    
C
无影响，因为模型已达100%准确率    
D
以上均不正确    
正确答案是：A    
增加模型复杂度会导致过拟合    

10、想象一下，机器学习中有1000个输入特征和1个目标特征，必须根据输入特征和目标特征之间的关系选择100个最重要的特征。你认为这是减少维数的例子吗？    
A
是    
B
不是     
正确答案是：A    

# 第十四天
1、判断：没有必要有一个用于应用维数降低算法的目标变量。    
A
真    
B
假    
正确答案是：A    

2、在数据集中有4个变量，如A，B，C和D.执行了以下操作：    
步骤1：使用上述变量创建另外两个变量，即E = A + 3 * B和F = B + 5 * C + D。    
步骤2：然后只使用变量E和F建立了一个随机森林模型。     

上述步骤可以表示降维方法吗？    
A
真    
B
假    
正确答案是：A    
因为步骤1可以用于将数据表示为2个较低的维度。    
3、以下哪种技术对于减少数据集的维度会更好？    
A
删除缺少值太多的列    
B
删除数据差异较大的列    
C
删除不同数据趋势的列    
D
都不是    
正确答案是：A    
如果列的缺失值太多（例如99％），那么可以删除这些列。    

4、判断：降维算法是减少构建模型所需计算时间的方法之一。    
A
真    
B  假    
正确答案是：    
降低数据维数将花费更少的时间来训练模型。    
5、以下哪种算法不能用于降低数据的维数？     
A
t-SNE    
B
PCA    
C
LDA    
D
都不是    
正确答案是：D    
所有算法都是降维算法的例子。    
6、判断：PCA可用于在较小维度上投影和可视化数据。    
A
真    
B
假      
正确答案是：A    
有时绘制较小维数据非常有用，可以使用前两个主要分量，然后使用散点图可视化数据。    
7、最常用的降维算法是PCA，以下哪项是关于PCA的？    
1.PCA是一种无监督的方法    
2.它搜索数据具有最大差异的方向    
3.主成分的最大数量<=特征能数量    
4.所有主成分彼此正交    
A
2、3和4    
B
1、2和3    
C
1、2和4     
D
以上所有    
正确答案是：D    

8、假设使用维数降低作为预处理技术，使用PCA将数据减少到k维度。然后使用这些PCA预测作为特征，以下哪个声明是正确的？    
A
更高的“k”意味着更正则化    
B
更高的“k”意味着较少的正则化    
C
都不对    
正确答案是： B    
较高的k导致较少的平滑，因此能够保留更多的数据特征，从而减少正则化。     
9、在相同的机器上运行并设置最小的计算能力，以下哪种情况下t-SNE比PCA降维效果更好？    
A
具有1百万项300个特征的数据集    
B
具有100000项310个特征的数据集    
C
具有10,000项8个特征的数据集    
D
具有10,000项200个特征的数据集    
正确答案是：C    
t-SNE具有二次时空复杂度。
10、对于t-SNE代价函数，以下陈述中的哪一个正确？     
A
本质上是不对称的    
B
本质上是对称的    
C 
与SNE的代价函数相同    
正确答案是： B    
SNE代价函数是不对称的，这使得使用梯度下降难以收敛。对称是SNE和t-SNE代价函数之间的主要区别之一。     
## 第15天
1、想像正在处理文本数据，使用单词嵌入（Word2vec）表示使用的单词。在单词嵌入中，最终会有1000维。现在想减小这个高维数据的维度，这样相似的词应该在最邻近的空间中具有相似的含义。在这种情况下，您最有可能选择以下哪种算法？    
A
t-SNE    
B
PCA    
C
LDA    
D
都不是    
正确答案是：A    
-SNE代表t分布随机相邻嵌入，它考虑最近的邻居来减少数据。
2、判断：t-SNE学习非参数映射。    
A
真    
B
假    
正确答案是：A    
t-SNE学习非参数映射，这意味着它不会学习将数据从输入空间映射到地图的显式函数。    
3、以下对于t-SNE和PCA的陈述中哪个是正确的？    
A
t-SNE是线性的，而PCA是非线性的    
B
t-SNE和PCA都是线性的    
C
t-SNE和PCA都是非线性的    
D
t-SNE是非线性的，而PCA是线性的    
正确答案是：D
4、在t-SNE算法中，可以调整以下哪些超参数？    
A
维度数量    
B
平稳测量有效数量的邻居    
C
最大迭代次数    
D
以上所有
正确答案是：D
5、与PCA相比，t-SNE的以下说明哪个正确？     
A
数据巨大（大小）时，t-SNE可能无法产生更好的结果。    
B
无论数据的大小如何，T-NSE总是产生更好的结果。    
C
对于较小尺寸的数据，PCA总是比t-SNE更好。    
D
都不是    
正确答案是：A    

6、Xi和Xj是较高维度表示中的两个不同点，其中Yi和Yj是较低维度中的Xi和Xj的表示。    
1.数据点Xi与数据点Xj的相似度是条件概率p（j | i）。    
2.数据点Yi与数据点Yj的相似度是条件概率q（j | i）。    
对于在较低维度空间中的Xi和Xj的完美表示，以下哪一项必须是正确的？    
A
p（j | i）= 0，q（j | i）= 1    
B
p（j | i）    
C
p（j | i）= q（j | i）    
D
P（j | i）> q（j | i）    
正确答案是：C    
两点的相似性的条件概率必须相等，因为点之间的相似性必须在高维和低维中保持不变，以使它们成为完美的表示
7、对于投影数据为(( √2)，(0)，(√2))。现在如果在二维空间中重建，并将它们视为原始数据点的重建，那么重建误差是多少？    
A
0％    
B
10％     
C
30％    
D
40％    
正确答案是：A    
重建误差为0，因为所有三个点完全位于第一个主要分量的方向上或者计算重建;
8、LDA的以下哪项是正确的？    
![](https://ws1.sinaimg.cn/large/006tNc79ly1fvtz07u5yej30c506uwet.jpg)    
A
LDA旨在最大化之间类别的距离，并最小化类内之间的距离    
B
LDA旨在最小化类别和类内之间的距离    
C
LDA旨在最大化类内之间的距离，并最小化类别之间的距离    
D
LDA旨在最大化类别和类内之间的距离    
正确答案是：A
9、LDA的思想是找到最能区分两类别之间的线，下图中哪个是好的投影？    
![](https://ws3.sinaimg.cn/large/006tNc79ly1fvtz7pbirlj30g609xt97.jpg)    
A
LD1    
B
LD2    
C
两者    
D
都不是    
正确答案是：A
10、以下哪种情况LDA会失败？    
A
如果有辨识性的信息不是平均值，而是数据的方差    
B
如果有辨识性的信息是平均值，而不是数据方差    
C
如果有辨识性的信息是数据的均值和方差    
D
都不是    
正确答案是：A
LDA算法既可以用来降维，又可以用来分类，但是目前来说，主要还是用于降维。在我们进行图像识别图像识别相关的数据分析时，LDA是一个有力的工具。下面总结下LDA算法的优缺点。

　　　　LDA算法的主要优点有：

　　　　1）在降维过程中可以使用类别的先验知识经验，而像PCA这样的无监督学习则无法使用类别先验知识。

　　　　2）LDA在样本分类信息依赖均值而不是方差的时候，比PCA之类的算法较优。

　　　　LDA算法的主要缺点有：

　　　　1）LDA不适合对非高斯分布样本进行降维，PCA也有这个问题。

　　　　2）LDA降维最多降到类别数k-1的维数，如果我们降维的维度大于k-1，则不能使用LDA。当然目前有一些LDA的进化版算法可以绕过这个问题。

　　　　3）LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好。

　　　　4）LDA可能过度拟合数据。

　　LDA用于降维，和PCA有很多相同，也有很多不同的地方，因此值得好好的比较一下两者的降维异同点。

　　　　首先我们看看相同点：

　　　　1）两者均可以对数据进行降维。

　　　　2）两者在降维时均使用了矩阵特征分解的思想。

　　　　3）两者都假设数据符合高斯分布。

　　　　我们接着看看不同点：

　　　　1）LDA是有监督的降维方法，而PCA是无监督的降维方法

　　　　2）LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。

　　　　3）LDA除了可以用于降维，还可以用于分类。

　　　　4）LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。

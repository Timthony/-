# 两个月刷完机器学习（包含深度学习）600题。（题目来源于BAT等厂面试题）  每天10道。    
## 第一天:    
1、下列时间序列模型中,哪一个模型可以较好地拟合波动性的分析和预测。    
A AR模型    
B MA模型    
C ARMA模型    
D GARCH模型        
解析：AR auto regressive model AR模型是一种线性预测
MA模型(moving average model)滑动平均模型，其中使用趋势移动平均法建立直线趋势的预测模型。
ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐。    
GARCH模型称为广义ARCH模型，是ARCH模型的拓展，由Bollerslev(1986)发展起来的。它是ARCH模型的推广。GARCH(p,0)模型，相当于ARCH(p)模型。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测。    
正确答案 D    
2、以下说法中错误的是（）    
A  SVM对噪声（如来自其他分部的噪声样本）具备鲁棒性    
B  在adaboost算法中，所有被分错样本的权重更新比例不相同     
C  boosting和bagging都是组合多个分类器投票的方法，二者都是根据单个分类器的正确率确定其权重   
D  给定n个数据点，如果其中一半用于训练，一半用户测试，则训练误差和测试误差之间的差别会随着n的增加而减少的    
解析：    
A   软间隔分类器对噪声是有鲁棒性的    
B   具体说来，整个Adaboost 迭代算法就3步：    
初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。
训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。    
将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。    
C    boosting是根据分类器正确率确定权重，bagging不是。    
Bagging即套袋法，其算法过程如下：    
A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）    
B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）   
C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）
Boosting其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。    
关于Boosting的两个核心问题：    
1）在每一轮如何改变训练数据的权值或概率分布？
通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。    
2）通过什么方式来组合弱分类器？    
通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。
而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。    
D    训练集变大会提高模型鲁棒性。    
正确答案C    
3、你正在使用带有 L1 正则化的 logistic 回归做二分类，其中 C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。当你把 C 值从 0 增加至非常大的值时，下面哪个选项是正确的？    
![image](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1514624605_280.png)
A 第一个 w2 成了 0，接着 w1 也成了 0    
B 第一个 w1 成了 0，接着 w2 也成了 0    
C w1 和 w2 同时成了 0    
D 即使在 C 成为大值之后，w1 和 w2 都不能成 0    
解析：L1正则化的函数如图，所以w1和w2可以为0。同时w1和w2是对称的，不会导致一个为0另一个不为0的状态。    
正确答案D    
4、在 k-均值算法中，以下哪个选项可用于获得全局最小？    
A 尝试为不同的质心（centroid）初始化运行算法    
B 整迭代的次数   
C 找到集群的最佳数量    
D 以上所有   
解析：所有都可以用来调试以找到全局最小。    
正确答案D    
5、假设你使用 log-loss 函数作为评估标准。下面这些选项，哪些是对作为评估标准的 log-loss 的正确解释。    
A 如果一个分类器对不正确的分类很自信，log-loss 会严重的批评它   
B 对一个特别的观察而言，分类器为正确的类别分配非常小的概率，然后对 log-loss 的相应分布会非常大    
C log-loss 越低，模型越好   
D 以上都是   
解析：损失函数总结(https://blog.csdn.net/ZHANG781068447/article/details/82752598)
正确答案D
6、下面哪个选项中哪一项属于确定性算法？
A  PCA   
B  K-Means   
C  以上都不是   
解析：确定性算法表明在不同运行中，算法输出并不会改变。如果我们再一次运行算法，PCA 会得出相同的结果，而 k-means 不会。    
正确答案：A    
7、两个变量的 Pearson 相关性系数为零，但这两个变量的值同样可以相关。这句描述是正确还是错误？   
A 正确   
B 错误    
解析：
答案为（A）：Pearson相关系数只能衡量线性相关性，但无法衡量非线性关系。如y=x^2，x和y有很强的非线性关系。

8、下面哪个/些超参数的增加可能会造成随机森林数据过拟合？        
A 树的数量    
B 树的深度    
C 学习速率    
解析：   
答案为（B）：通常情况下，我们增加树的深度有可能会造成模型过拟合。学习速率并不是随机森林的超参数。增加树的数量可能会造成欠拟合。   
9、下列哪个不属于常用的文本分类的特征选择算法？    
A 卡方检验值    
B 互信息    
C 信息增益    
D 主成分分析    
解析：    
答案D    
常采用特征选择方法。常见的六种特征选择方法：    
1）DF(Document Frequency) 文档频率             
DF:统计特征词出现的文档数量，用来衡量某个特征词的重要性    
2）MI(Mutual Information) 互信息法    
互信息法用于衡量特征词与文档类别直接的信息量。
如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向"低频"的特征词。    
相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。    
3）(Information Gain) 信息增益法    
通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。    
4）CHI(Chi-square) 卡方检验法    
利用了统计学中的"假设检验"的基本思想：首先假设特征词与类别直接是不相关的    
如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。    
5）WLLR(Weighted Log Likelihood Ration)加权对数似然    
6）WFO（Weighted Frequency and Odds）加权频率和可能性    
10、机器学习中做特征选择时，可能用到的方法有？    
A 卡方    
B 信息增益
C 平均互信息    
D 期望交叉熵    
E 以上都有    
正确答案是：E        

## 第二天    
11、下列方法中，不可以用于特征降维的方法包括   
A 主成分分析PCA    
B 线性判别分析LDA    
C 深度学习SparseAutoEncoder    
D 矩阵奇异值分解SVD    
正确答案是：C    
特征降维方法主要有：    
PCA，LLE，Isomap
SVD和PCA类似，也可以看成一种降维方法
LDA:线性判别分析，可用于降维    
AutoEncoder：AutoEncoder的结构与神经网络的隐含层相同，由输入L1,输出  L2组成，中间则是权重连接。Autoencoder通过L2得到输入的重构L3，最小化L3与L1的差别  进行训练得到权重。在这样的权重参数下，得到的L2可以尽可能的保存L1的信息。     
Autoencoder的输出L2的维度由输出的神经元个数决定。当输出维度大于L1时，则需要在训练目标函数中加入sparse    惩罚项，避免L2直接复制L1（权重全为1）。所以称为sparseAutoencoder( Andrew Ng提出的)。
结论：SparseAutoencoder大多数情况下都是升维的，所以称之为特征降维的方法不准确。
![image](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1514641291_629.png)    
12、下列哪些不特别适合用来对高维数据进行降维     
A LASSO     
B 主成分分析法     
C 聚类分析     
D 小波分析法        
E 线性判别法    
F 拉普拉斯特征映射       
正确答案是：C
lasso通过参数缩减达到降维的目的；    
pca就不用说了     
线性鉴别法即LDA通过找到一个空间使得类内距离最小类间距离最大所以可以看做是降维；    
小波分析有一些变换的操作降低其他干扰可以看做是降维     
拉普拉斯请看这个http://f.dataguru.cn/thread-287243-1-1.html     
13、下列属于无监督学习的是    
A k-means    
B SVM    
C 最大熵    
D CRF    
正确答案：A     
 A是聚类，属于无监督学习。BC是分类，属于监督学习。至于D是序列化标注，也是有监督学习。    
14、下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）    
A 特征灵活    
B 速度快    
C 可容纳较多上下文信息    
D 全局最优     
正确答案是： B    
CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢
CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） ————与HMM比较    
同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­————与MEMM比较    
CRF是在给定需要标记的观察序列的条件下，使用维特比算法，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。————与ME比较    
15、以下哪个是常见的时间序列算法模型    
A RSI    
B MACD    
C ARMA    
D KDJ    
正确答案是：C    
自回归滑动平均模型(ARMA)     
其建模思想可概括为：逐渐增加模型的阶数，拟合较高阶模型，直到再增加模型的阶数而剩余残差方差不再显著减小为止。    
其他三项都不是一个层次的。     
A.相对强弱指数 (RSI, Relative Strength Index)     是通过比较一段时期内的平均收盘涨数和平均收盘跌数来分析市场买沽盘的意向和实力 , 从而作出未来市场的走势 .    
B.移动平均聚散指标 (MACD, Moving Average Convergence Divergence),     是根据均线的构造原理 , 对股票价格的收盘价进行平滑处理 , 求出算术平均值以后再进行计算 , 是一种趋向类指标 .    
D. 随机指标 (KDJ) 一般是根据统计学的原理 , 通过一个特定的周期 ( 常为 9 日 ,9 周等 ) 内出现过的最高价 , 最低价及最后一个计算周期的收盘价及这三者之间的比例关系 , 来计算最后一个计算周期的未成熟随机值 RSV, 然后根据平滑移动平均线的方法来计算 K 值 , D 值与 J 值 , 并绘成曲线图来研判股票走势 .    
16、下列不是SVM核函数的是    
A 多项式核函数    
B logistic核函数    
C 径向基核函数    
D Sigmoid核函数        
正确答案是： B    
SVM核函数包括线性核函数、多项式核函数、径向基核函数、高斯核函数、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数。
核函数的定义并不困难，根据泛函的有关理论，只要一种函数 K ( x i , x j ) 满足Mercer条件，它就对应某一变换空间的内积．对于判断哪些函数是核函数到目前为止也取得了重要的突破，得到Mercer定理和以下常用的核函数类型：    
(1)线性核函数     
K ( x , x i ) = x ⋅ x i    
(2)多项式核     
K ( x , x i ) = ( ( x ⋅ x i ) + 1 ) d     
(3)径向基核（RBF）    
K ( x , x i ) = exp ( − ∥ x − x i ∥ 2 σ 2 )     
Gauss径向基函数则是局部性强的核函数，其外推能力随着参数 σ 的增大而减弱。多项式形式的核函数具有良好的全局性质。局部性较差。    
(4)傅里叶核     
K ( x , x i ) = 1 − q 2 2 ( 1 − 2 q cos ( x − x i ) + q 2 )    
(5)样条核 
K ( x , x i ) = B 2 n + 1 ( x − x i )    
(6)Sigmoid核函数     
K ( x , x i ) = tanh ( κ ( x , x i ) − δ )    
采用Sigmoid函数作为核函数时，支持向量机实现的就是一种多层感知器神经网络，应用SVM方法，隐含层节点数目(它确定神经网络的结构)、隐含层节点对输入节点的权值都是在设计(训练)的过程中自动确定的。而且支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最小值，也保证了它对于未知样本的良好泛化能力而不会出现过学习现象。

核函数的选择    
在选取核函数解决实际问题时，通常采用的方法有：    
一是利用专家的先验知识预先选定核函数；    
二是采用Cross-Validation方法，即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数．如针对傅立叶核、RBF核，结合信号处理问题中的函数回归问题，通过仿真实验，对比分析了在相同数据条件下，采用傅立叶核的SVM要比采用RBF核的SVM误差小很多．    
三是采用由Smits等人提出的混合核函数方法，该方法较之前两者是目前选取核函数的主流方法，也是关于如何构造核函数的又一开创性的工作．将不同的核函数结合起来后会有更好的特性，这是混合核函数方法的基本思想．    
17、解决隐马模型中预测问题的算法是    
A 前向算法    
B 后向算法    
C Baum-Welch算法    
D 维特比算法    
正确答案是：D    
A、B：前向、后向算法解决的是一个评估问题，即给定一个模型，求某特定观测序列的概率，用于评估该序列最匹配的模型。    
C：Baum-Welch算法解决的是一个模型训练问题，即参数估计，是一种无监督的训练方法，主要通过EM迭代实现；     
D：维特比算法解决的是给定      一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。如通过海藻变化（输出序列）来观测天气（状态序列），是预测问题，通信中的解码问题。    
18、一般，k-NN最近邻方法在（）的情况下效果较好    
A 样本较多但典型性不好    
B 样本较少但典型性好    
C 样本呈团状分布    
D 样本呈链状分布    
正确答案是： B    
K近邻算法主要依靠的是周围的点，因此如果样本过多，那肯定是区分不出来的。因此应当选择B    
样本呈团状颇有迷惑性，这里应该指的是整个样本都是呈团状分布，这样kNN就发挥不出其求近邻的优势了，整体样本应该具有典型性好，样本较少，比较适宜。    
9、在一个n维的空间中， 最好的检测outlier(离群点)的方法是（）    
A 作正态分布概率图    
B 作盒形图    
C 马氏距离    
D 作散点图       
正确答案是：C    
解析：
![](https://ws3.sinaimg.cn/large/006tNbRwly1fvelohkwanj30la09aacb.jpg)    
20、对数几率回归（logistics regression）和一般回归分析有什么区别？    
A 对数几率回归是设计用来预测事件可能性的    
B 对数几率回归可以用来度量模型拟合程度    
C 对数几率回归可以用来估计回归系数    
D 以上所有    
正确答案是：D     
解析：    
A: 对数几率回归其实是设计用来解决分类问题的    
B: 对数几率回归可以用来检验模型对数据的拟合度    
C: 虽然对数几率回归是用来解决分类问题的，但是模型建立好后，就可以根据独立的特征，估计相关的回归系数。就我认为，这只是估计回归系数，不能直接用来做回归模型。  

## 第三天
31、bootstrap数据是什么意思？（提示：考“bootstrap”和“boosting”区别）    
A 有放回地从总共M个特征中抽样m个特征    
B 无放回地从总共M个特征中抽样m个特征    
C 有放回地从总共N个样本中抽样n个样本    
D 无放回地从总共N个样本中抽样n个样本    
正确答案是：C    
bootstrap其实就是bagging的意思，是Bootstrap Aggregating的缩写，根据定义就可知是有放回的采样，    
从m个样本的原始数据集里进行n（n<=m）次采样，构成一个包含n个样本的新训练数据集，然后拿这个新的数据集来训练模型。重复上述过程B次，得到B个模型，当有新的模型需要进行预测时，拿这B个模型分别对这个样本进行预测，然后采用投票的方式（分类问题）或求平均值（回归问题）得到新样本的预测值。    
32、“过拟合”只在监督学习中出现，在非监督学习中，没有“过拟合”，这是（）    
A 对的    
B 错的    
正确答案是： B    
我们可以评估无监督学习方法通过无监督学习的指标    
33、对于k折交叉验证, 以下对k的说法正确的是（）    
A k越大, 不一定越好, 选择大的k会加大评估时间    
B 选择更大的k, 就会有更小的bias (因为训练集更加接近总数据集)    
C 在选择k时, 要最小化数据集之间的方差    
D 以上所有    
正确答案：D    
k越大, bias越小, 训练时间越长. 在训练时, 也要考虑数据集间方差差别不大的原则. 比如, 对于二类分类问题, 使用2-折交叉验证, 如果测试集里的数据都是A类的, 而训练集中数据都是B类的, 显然, 测试效果会很差.    
34、回归模型中存在多重共线性, 你如何解决这个问题？    
1 去除这两个共线性变量    
2 我们可以先去除一个共线性变量        
3 计算VIF(方差膨胀因子), 采取相应措施        
4 为了避免损失信息, 我们可以使用一些正则化方法, 比如, 岭回归和lasso回归       
A 1    
B 2    
C 2和3    
D 2, 3和4    
正确答案是：D    
解决多重公线性, 可以使用相关矩阵去去除相关性高于75%的变量 (有主观成分). 也可以VIF, 如果VIF值<=4说明相关性不是很高, VIF值>=10说明相关性较高.
我们也可以用 岭回归和lasso回归的带有惩罚正则项的方法. 我们也可以在一些变量上加随机噪声, 使得变量之间变得不同, 但是这个方法要小心使用, 可能会影响预测效果。    
35、模型的高bias是什么意思, 我们如何降低它 ？    
A 在特征空间中减少特征    
B 在特征空间中增加特征    
C 增加数据点    
D B和C    
E 以上所有    
正确答案是： B    
bias太高说明模型太简单了, 数据维数不够, 无法准确预测数据, 所以, 升维吧 !    
36、训练决策树模型, 属性节点的分裂, 具有最大信息增益的图是下图的哪一个（）    
![](https://ws1.sinaimg.cn/large/006tNbRwly1fvf7rrms94j30c909eq40.jpg)    
A Outlook    
B Humidity    
C Windy    
D Temperature    
正确答案是：A    
根据信息增益的定义计算可得。    
37、对于信息增益, 决策树分裂节点, 下面说法正确的是（）     
1 纯度高的节点需要更多的信息去区分    
2 信息增益可以用”1比特-熵”获得    
3 如果选择一个属性具有许多归类值, 那么这个信息增益是有偏差的    
A 1    
B 2    
C 2和3    
D 所有以上    
正确答案是：C    
纯度越高，表示不确定越少，更少的信息就可以区分    
38、下图是同一个SVM模型, 但是使用了不同的径向基核函数的gamma参数, 依次是g1, g2, g3 , 下面大小比较正确的是    
![](https://ws4.sinaimg.cn/large/006tNbRwly1fvf7w94cxej30gi07fjs0.jpg)    
A g1 > g2 > g3    
B g1 = g2 = g3    
C g1 < g2 < g3    
D g1 >= g2 >= g3E. g1 <= g2 <= g3    
正确答案是：C    
所谓径向基函数 (Radial Basis Function 简称 RBF), 就是某种沿径向对称的标量函数。 通常定义为空间中任一点x到某一中心点xc之间欧氏距离的单调函数 , 可记作 k(||x-xc||), 其作用往往是局部的 , 即当x远离xc时函数取值很小。最常用的径向基函数是高斯核函数 ,形式为 k(||x-xc||)=exp{- ||x-xc||^2/(2*σ^2) } 其中xc为核函数中心,σ为函数的宽度参数 , 控制了函数的径向作用范围。由radial basis: exp(-gamma*|u-v|^2)可知, gamma越小, 模型越简单, 平滑度越好, 分类边界越不容易过拟合, 所以选C。    
39、假设我们要解决一个二类分类问题, 我们已经建立好了模型, 输出是0或1, 初始时设阈值为0.5, 超过0.5概率估计, 就判别为1, 否则就判别为0 ; 如果我们现在用另一个大于0.5的阈值,  那么现在关于模型说法, 正确的是 :     
1 模型分类的召回率会降低或不变    
2 模型分类的召回率会升高    
3 模型分类准确率会升高或不变    
4 模型分类准确率会降低    
A 1    
B 2    
C 1和3    
D 2和4    
E 以上都不是    
正确答案是：A    
精确率, 准确率和召回率是广泛用于信息检索和统计学分类领域的度量值，用来评价结果的质量。下图可以帮助理解和记忆它们之间的关系, 其中精确率(precision)和准确率(accuracy)都是关于预测效果的描述. 召回率是关于预测样本的描述。
精确率表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP), 也就是P = TP / (TP + FP)。

准确率表示的是预测的正负样本有多少是真实的正和负, 预测正确的数量占全部预测数量的比例, 也就是A = (TP + TN) / (TP + FP + TN + FN) = (TP + TN) / 全部样本。

召回率表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN), 也就是R = TP / (TP + FN)。
精确率和召回率二者计算方法其实就是分母不同，一个分母是预测为正的样本数，另一个是原来样本中所有的正样本数。
提高分界阈值大于0.5, 则预测为正的样本数要降低, 相当于把图中圆圈变小, 按下图则可计算    
![](https://ws1.sinaimg.cn/large/006tNbRwly1fvf80ecoo7j30du0sbmyp.jpg)    
召回率的分子变小分母不变, 所以召回率会变小或不变;
精确率的分子分母同步变化, 所以精确率的变化不能确定;
准确率的分子为圆内绿色加圆外右侧矩形面积所围样本, 两者之和变化不能确定; 分母为矩形所含全部样本不变化, 所以准确率的变化不能确定;
综上, 所以选A。    
40、“点击率问题”是这样一个预测问题, 99%的人是不会点击的, 而1%的人是会点击进去的, 所以这是一个非常不平衡的数据集. 假设, 现在我们已经建了一个模型来分类, 而且有了99%的预测准确率, 我们可以下的结论是        
A 模型预测准确率已经很高了, 我们不需要做什么了    
B 模型预测准确率不高, 我们需要做点什么改进模型    
C 无法下结论    
D 以上都不对    
正确答案是：C    
类别不均衡的情况下，不要用准确率做分类评估指标，因为全判断为不会点，准确率也是99%，但是这个分类器一点用都没有。
此时应该用查准率或查全率，更加能反映情况。    


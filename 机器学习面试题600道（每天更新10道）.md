# 两个月刷完机器学习（包含深度学习）600题。（题目来源于BAT等厂面试题）  每天10道。    
## 第一天:    
1、下列时间序列模型中,哪一个模型可以较好地拟合波动性的分析和预测。    
A AR模型    
B MA模型    
C ARMA模型    
D GARCH模型        
解析：AR auto regressive model AR模型是一种线性预测
MA模型(moving average model)滑动平均模型，其中使用趋势移动平均法建立直线趋势的预测模型。
ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐。    
GARCH模型称为广义ARCH模型，是ARCH模型的拓展，由Bollerslev(1986)发展起来的。它是ARCH模型的推广。GARCH(p,0)模型，相当于ARCH(p)模型。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测。    
正确答案 D    
2、以下说法中错误的是（）    
A  SVM对噪声（如来自其他分部的噪声样本）具备鲁棒性    
B  在adaboost算法中，所有被分错样本的权重更新比例不相同     
C  boosting和bagging都是组合多个分类器投票的方法，二者都是根据单个分类器的正确率确定其权重   
D  给定n个数据点，如果其中一半用于训练，一半用户测试，则训练误差和测试误差之间的差别会随着n的增加而减少的    
解析：    
A   软间隔分类器对噪声是有鲁棒性的    
B   具体说来，整个Adaboost 迭代算法就3步：    
初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。
训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。    
将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。    
C    boosting是根据分类器正确率确定权重，bagging不是。    
Bagging即套袋法，其算法过程如下：    
A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）    
B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）   
C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）
Boosting其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。    
关于Boosting的两个核心问题：    
1）在每一轮如何改变训练数据的权值或概率分布？
通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。    
2）通过什么方式来组合弱分类器？    
通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。
而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。    
D    训练集变大会提高模型鲁棒性。    
正确答案C    
3、你正在使用带有 L1 正则化的 logistic 回归做二分类，其中 C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。当你把 C 值从 0 增加至非常大的值时，下面哪个选项是正确的？    
![image](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1514624605_280.png)
A 第一个 w2 成了 0，接着 w1 也成了 0    
B 第一个 w1 成了 0，接着 w2 也成了 0    
C w1 和 w2 同时成了 0    
D 即使在 C 成为大值之后，w1 和 w2 都不能成 0    
解析：L1正则化的函数如图，所以w1和w2可以为0。同时w1和w2是对称的，不会导致一个为0另一个不为0的状态。    
正确答案D    
4、在 k-均值算法中，以下哪个选项可用于获得全局最小？    
A 尝试为不同的质心（centroid）初始化运行算法    
B 整迭代的次数   
C 找到集群的最佳数量    
D 以上所有   
解析：所有都可以用来调试以找到全局最小。    
正确答案D    
5、假设你使用 log-loss 函数作为评估标准。下面这些选项，哪些是对作为评估标准的 log-loss 的正确解释。    
A 如果一个分类器对不正确的分类很自信，log-loss 会严重的批评它   
B 对一个特别的观察而言，分类器为正确的类别分配非常小的概率，然后对 log-loss 的相应分布会非常大    
C log-loss 越低，模型越好   
D 以上都是   
解析：损失函数总结(https://blog.csdn.net/ZHANG781068447/article/details/82752598)
正确答案D
6、下面哪个选项中哪一项属于确定性算法？
A  PCA   
B  K-Means   
C  以上都不是   
解析：确定性算法表明在不同运行中，算法输出并不会改变。如果我们再一次运行算法，PCA 会得出相同的结果，而 k-means 不会。    
正确答案：A    
7、两个变量的 Pearson 相关性系数为零，但这两个变量的值同样可以相关。这句描述是正确还是错误？   
A 正确   
B 错误    
解析：
答案为（A）：Pearson相关系数只能衡量线性相关性，但无法衡量非线性关系。如y=x^2，x和y有很强的非线性关系。

8、下面哪个/些超参数的增加可能会造成随机森林数据过拟合？        
A 树的数量    
B 树的深度    
C 学习速率    
解析：   
答案为（B）：通常情况下，我们增加树的深度有可能会造成模型过拟合。学习速率并不是随机森林的超参数。增加树的数量可能会造成欠拟合。   
9、下列哪个不属于常用的文本分类的特征选择算法？    
A 卡方检验值    
B 互信息    
C 信息增益    
D 主成分分析    
解析：    
答案D    
常采用特征选择方法。常见的六种特征选择方法：    
1）DF(Document Frequency) 文档频率             
DF:统计特征词出现的文档数量，用来衡量某个特征词的重要性    
2）MI(Mutual Information) 互信息法    
互信息法用于衡量特征词与文档类别直接的信息量。
如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向"低频"的特征词。    
相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。    
3）(Information Gain) 信息增益法    
通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。    
4）CHI(Chi-square) 卡方检验法    
利用了统计学中的"假设检验"的基本思想：首先假设特征词与类别直接是不相关的    
如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。    
5）WLLR(Weighted Log Likelihood Ration)加权对数似然    
6）WFO（Weighted Frequency and Odds）加权频率和可能性    
10、机器学习中做特征选择时，可能用到的方法有？    
A 卡方    
B 信息增益
C 平均互信息    
D 期望交叉熵    
E 以上都有    
正确答案是：E        

## 第二天    
11、下列方法中，不可以用于特征降维的方法包括   
A 主成分分析PCA    
B 线性判别分析LDA    
C 深度学习SparseAutoEncoder    
D 矩阵奇异值分解SVD    
正确答案是：C    
特征降维方法主要有：    
PCA，LLE，Isomap
SVD和PCA类似，也可以看成一种降维方法
LDA:线性判别分析，可用于降维    
AutoEncoder：AutoEncoder的结构与神经网络的隐含层相同，由输入L1,输出  L2组成，中间则是权重连接。Autoencoder通过L2得到输入的重构L3，最小化L3与L1的差别  进行训练得到权重。在这样的权重参数下，得到的L2可以尽可能的保存L1的信息。     
Autoencoder的输出L2的维度由输出的神经元个数决定。当输出维度大于L1时，则需要在训练目标函数中加入sparse    惩罚项，避免L2直接复制L1（权重全为1）。所以称为sparseAutoencoder( Andrew Ng提出的)。
结论：SparseAutoencoder大多数情况下都是升维的，所以称之为特征降维的方法不准确。
![image](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1514641291_629.png)    
12、下列哪些不特别适合用来对高维数据进行降维     
A LASSO     
B 主成分分析法     
C 聚类分析     
D 小波分析法        
E 线性判别法    
F 拉普拉斯特征映射       
正确答案是：C
lasso通过参数缩减达到降维的目的；    
pca就不用说了     
线性鉴别法即LDA通过找到一个空间使得类内距离最小类间距离最大所以可以看做是降维；    
小波分析有一些变换的操作降低其他干扰可以看做是降维     
拉普拉斯请看这个http://f.dataguru.cn/thread-287243-1-1.html     
13、下列属于无监督学习的是    
A k-means    
B SVM    
C 最大熵    
D CRF    
正确答案：A     
 A是聚类，属于无监督学习。BC是分类，属于监督学习。至于D是序列化标注，也是有监督学习。    
14、下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）    
A 特征灵活    
B 速度快    
C 可容纳较多上下文信息    
D 全局最优     
正确答案是： B    
CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢
CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） ————与HMM比较    
同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­————与MEMM比较    
CRF是在给定需要标记的观察序列的条件下，使用维特比算法，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。————与ME比较    
15、以下哪个是常见的时间序列算法模型    
A RSI    
B MACD    
C ARMA    
D KDJ    
正确答案是：C    
自回归滑动平均模型(ARMA)     
其建模思想可概括为：逐渐增加模型的阶数，拟合较高阶模型，直到再增加模型的阶数而剩余残差方差不再显著减小为止。    
其他三项都不是一个层次的。     
A.相对强弱指数 (RSI, Relative Strength Index)     是通过比较一段时期内的平均收盘涨数和平均收盘跌数来分析市场买沽盘的意向和实力 , 从而作出未来市场的走势 .    
B.移动平均聚散指标 (MACD, Moving Average Convergence Divergence),     是根据均线的构造原理 , 对股票价格的收盘价进行平滑处理 , 求出算术平均值以后再进行计算 , 是一种趋向类指标 .    
D. 随机指标 (KDJ) 一般是根据统计学的原理 , 通过一个特定的周期 ( 常为 9 日 ,9 周等 ) 内出现过的最高价 , 最低价及最后一个计算周期的收盘价及这三者之间的比例关系 , 来计算最后一个计算周期的未成熟随机值 RSV, 然后根据平滑移动平均线的方法来计算 K 值 , D 值与 J 值 , 并绘成曲线图来研判股票走势 .    
16、下列不是SVM核函数的是    
A 多项式核函数    
B logistic核函数    
C 径向基核函数    
D Sigmoid核函数        
正确答案是： B    
SVM核函数包括线性核函数、多项式核函数、径向基核函数、高斯核函数、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数。
核函数的定义并不困难，根据泛函的有关理论，只要一种函数 K ( x i , x j ) 满足Mercer条件，它就对应某一变换空间的内积．对于判断哪些函数是核函数到目前为止也取得了重要的突破，得到Mercer定理和以下常用的核函数类型：    
(1)线性核函数     
K ( x , x i ) = x ⋅ x i    
(2)多项式核     
K ( x , x i ) = ( ( x ⋅ x i ) + 1 ) d     
(3)径向基核（RBF）    
K ( x , x i ) = exp ( − ∥ x − x i ∥ 2 σ 2 )     
Gauss径向基函数则是局部性强的核函数，其外推能力随着参数 σ 的增大而减弱。多项式形式的核函数具有良好的全局性质。局部性较差。    
(4)傅里叶核     
K ( x , x i ) = 1 − q 2 2 ( 1 − 2 q cos ( x − x i ) + q 2 )    
(5)样条核 
K ( x , x i ) = B 2 n + 1 ( x − x i )    
(6)Sigmoid核函数     
K ( x , x i ) = tanh ( κ ( x , x i ) − δ )    
采用Sigmoid函数作为核函数时，支持向量机实现的就是一种多层感知器神经网络，应用SVM方法，隐含层节点数目(它确定神经网络的结构)、隐含层节点对输入节点的权值都是在设计(训练)的过程中自动确定的。而且支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最小值，也保证了它对于未知样本的良好泛化能力而不会出现过学习现象。

核函数的选择    
在选取核函数解决实际问题时，通常采用的方法有：    
一是利用专家的先验知识预先选定核函数；    
二是采用Cross-Validation方法，即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数．如针对傅立叶核、RBF核，结合信号处理问题中的函数回归问题，通过仿真实验，对比分析了在相同数据条件下，采用傅立叶核的SVM要比采用RBF核的SVM误差小很多．    
三是采用由Smits等人提出的混合核函数方法，该方法较之前两者是目前选取核函数的主流方法，也是关于如何构造核函数的又一开创性的工作．将不同的核函数结合起来后会有更好的特性，这是混合核函数方法的基本思想．    
17、解决隐马模型中预测问题的算法是    
A 前向算法    
B 后向算法    
C Baum-Welch算法    
D 维特比算法    
正确答案是：D    
A、B：前向、后向算法解决的是一个评估问题，即给定一个模型，求某特定观测序列的概率，用于评估该序列最匹配的模型。    
C：Baum-Welch算法解决的是一个模型训练问题，即参数估计，是一种无监督的训练方法，主要通过EM迭代实现；     
D：维特比算法解决的是给定      一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。如通过海藻变化（输出序列）来观测天气（状态序列），是预测问题，通信中的解码问题。    
18、一般，k-NN最近邻方法在（）的情况下效果较好    
A 样本较多但典型性不好    
B 样本较少但典型性好    
C 样本呈团状分布    
D 样本呈链状分布    
正确答案是： B    
K近邻算法主要依靠的是周围的点，因此如果样本过多，那肯定是区分不出来的。因此应当选择B    
样本呈团状颇有迷惑性，这里应该指的是整个样本都是呈团状分布，这样kNN就发挥不出其求近邻的优势了，整体样本应该具有典型性好，样本较少，比较适宜。    
9、在一个n维的空间中， 最好的检测outlier(离群点)的方法是（）    
A 作正态分布概率图    
B 作盒形图    
C 马氏距离    
D 作散点图       
正确答案是：C    
解析：
![](https://ws3.sinaimg.cn/large/006tNbRwly1fvelohkwanj30la09aacb.jpg)    
20、对数几率回归（logistics regression）和一般回归分析有什么区别？    
A 对数几率回归是设计用来预测事件可能性的    
B 对数几率回归可以用来度量模型拟合程度    
C 对数几率回归可以用来估计回归系数    
D 以上所有    
正确答案是：D     
解析：    
A: 对数几率回归其实是设计用来解决分类问题的    
B: 对数几率回归可以用来检验模型对数据的拟合度    
C: 虽然对数几率回归是用来解决分类问题的，但是模型建立好后，就可以根据独立的特征，估计相关的回归系数。就我认为，这只是估计回归系数，不能直接用来做回归模型。     
